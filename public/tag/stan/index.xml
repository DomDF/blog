<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stan | All Your Bayes</title>
    <link>/tag/stan/</link>
      <atom:link href="/tag/stan/index.xml" rel="self" type="application/rss+xml" />
    <description>Stan</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><lastBuildDate>Tue, 28 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Stan</title>
      <link>/tag/stan/</link>
    </image>
    
    <item>
      <title>Maximum Likelihood Estimation</title>
      <link>/post/2020-07-28-maximum-likelihood-estimation/</link>
      <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-07-28-maximum-likelihood-estimation/</guid>
      <description>


&lt;div id=&#34;tldr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TLDR&lt;/h3&gt;
&lt;p&gt;Maximum Likelihood Estimation (MLE) is one method of inferring model parameters. This post aims to give an intuitive explanation of MLE, discussing why it is so useful (simplicity and availability in software) as well as where it is limited (point estimates are not as informative as Bayesian estimates, which are also shown for comparison).&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Distribution parameters describe the shape of a distribution function. A normal (Gaussian) distribution is characterised based on it’s mean, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and standard deviation, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Increasing the mean &lt;em&gt;shifts&lt;/em&gt; the distribution to be centered at a larger value and increasing the standard deviation &lt;em&gt;stretches&lt;/em&gt; the function to give larger values further away from the mean. When we approximate some uncertain data with a distribution function, we are interested in estimating the distribution parameters that are most consistent with the data.&lt;/p&gt;
&lt;p&gt;The likelihood, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;, of some data, &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, is shown below. Where &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt; is the function that has been proposed to explain the data, and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are the parameter(s) that characterise that function.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L = \displaystyle\prod_{i=1}^{N} f(z_{i} \mid \theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Likelihood values (and therefore also the product of many likelihood values) can be very small, so small that they cause problems for software. Therefore it’s usually more convenient to work with log-likelihoods instead. Taking the logarithm is applying a &lt;em&gt;monotonically increasing&lt;/em&gt; function. This means if one function has a higher sample likelihood than another, then it will also have a higher log-likelihood. Also, the location of maximum log-likelihood will be also be the location of the maximum likelihood.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\log{(L)} = \displaystyle\sum_{i=1}^{N} f(z_{i} \mid \theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The distribution parameters that maximise the log-likelihood function, &lt;span class=&#34;math inline&#34;&gt;\(\theta^{*}\)&lt;/span&gt;, are those that correspond to the maximum sample likelihood.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta^{*} = arg \max_{\theta} \bigg[ \log{(L)} \bigg]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Below, two different normal distributions are proposed to describe a pair of observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obs &amp;lt;- c(0, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The red distribution has a mean value of &lt;code&gt;1&lt;/code&gt; and a standard deviation of &lt;code&gt;2&lt;/code&gt;. The green distribution has a mean value of &lt;code&gt;2&lt;/code&gt; and a standard deviation of &lt;code&gt;1&lt;/code&gt; and so is centered further to the right, and is less dispersed (less stretched out). The red arrows point to the likelihood values of the data associated with the red distribution, and the green arrows indicate the likelihood of the same data with respect to the green function. The first data point, 0 is more likely to have been generated by the red function, and the second data point, 3 is more likely to have been generated by the green function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-maximum-likelihood-estimation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can evaluate the log-likelihood and compare the two functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(log(dnorm(x = obs, mean = 1, sd = 2))) # Red function
## [1] -3.849171

sum(log(dnorm(x = obs, mean = 2, sd = 1))) # Green function
## [1] -4.337877&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As shown above, the red distribution has a higher log-likelihood (and therefore also a higher likelihood) than the green function, with respect to the 2 data points. The above graph suggests that this is driven by the first data point , 0 being significantly more consistent with the red function. The below example looks at how a distribution parameter that maximises a sample likelihood could be identified.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mle-for-an-exponential-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MLE for an Exponential Distribution&lt;/h3&gt;
&lt;p&gt;The exponential distribution is characterised by a single parameter, it’s rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(z, \lambda) = \lambda \cdot \exp^{- \lambda \cdot z}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is a widely used distribution, as it is a Maximum Entropy (MaxEnt) solution. If some unknown parameters is known to be positive, with a fixed mean, then the function that best conveys this (and only this) information is the exponential distribution. I plan to write a future post about the MaxEnt principle, as it is deeply linked to Bayesian statistics. The expectation (mean), &lt;span class=&#34;math inline&#34;&gt;\(E[y]\)&lt;/span&gt; and variance, &lt;span class=&#34;math inline&#34;&gt;\(Var[y]\)&lt;/span&gt; of an exponentially distributed parameter, &lt;span class=&#34;math inline&#34;&gt;\(y \sim exp(\lambda)\)&lt;/span&gt; are shown below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[y] = \lambda^{-1}, \; Var[y] = \lambda^{-2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Simulating some example data…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_samples &amp;lt;- 25; true_rate &amp;lt;- 1; set.seed(1)

exp_samples &amp;lt;- rexp(n = n_samples,
                    rate = true_rate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, 25 independent random samples have been taken from an exponential distribution with a mean of 1, using &lt;code&gt;rexp&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Below, for various proposed &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values, the log-likelihood (&lt;code&gt;log(dexp())&lt;/code&gt;) of the sample is evaluated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
exp_lik_df &amp;lt;- data.frame(rate = double(), 
                         lik = double())

for (i in seq(from = 0.2, to = 2, by = 0.2)){
  
  exp_lik_df &amp;lt;- rbind(exp_lik_df, 
                      data.frame(rate = i,
                                 log_lik = sum(log(
                                   dexp(x = exp_samples,
                                        rate = i)))))
  
}

max_log_lik &amp;lt;- exp_lik_df[which.max(x = exp_lik_df$log_lik),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, &lt;code&gt;max_log_lik&lt;/code&gt; finds which of the proposed &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values is associated with the highest log-likelihood.&lt;/p&gt;
&lt;p&gt;We can print out the data frame that has just been created and check that the maximum has been correctly identified.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(exp_lik_df)
##    rate   log_lik
## 1   0.2 -45.38774
## 2   0.4 -33.21086
## 3   0.6 -28.22602
## 4   0.8 -26.18577
## 5   1.0 -25.75897
## 6   1.2 -26.35273
## 7   1.4 -27.65076
## 8   1.6 -29.46427
## 9   1.8 -31.67149
## 10  2.0 -34.18927

print(max_log_lik)
##   rate   log_lik
## 5    1 -25.75897&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The below plot shows how the sample log-likelihood varies for different values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. It also shows the shape of the exponential distribution associated with the lowest (top-left), optimal (top-centre) and highest (top-right) values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; considered in these iterations:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-maximum-likelihood-estimation_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;mle-in-practice-software-libraries&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;MLE in Practice: Software Libraries&lt;/h4&gt;
&lt;p&gt;In practice there are many software packages that quickly and conveniently automate MLE. Here are some useful examples…&lt;/p&gt;
&lt;p&gt;Firstly, using the &lt;code&gt;fitdistrplus&lt;/code&gt; library in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
library(fitdistrplus)

sample_data &amp;lt;- exp_samples

rate_fit_R &amp;lt;- fitdist(data = sample_data, 
                      distr = &amp;#39;exp&amp;#39;, 
                      method = &amp;#39;mle&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although I have specified &lt;code&gt;mle&lt;/code&gt; (maximum likelihood estimation) as the method that I would like &lt;code&gt;R&lt;/code&gt; to use here, it is already the default argument and so we didn’t need to include it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; provides us with an list of plenty of useful information, including:
- the original data
- the size of the dataset
- the co-variance matrix (especially useful if we are estimating multiple parameters)
- some measures of well the parameters were estimated&lt;/p&gt;
&lt;p&gt;You can explore these using &lt;code&gt;$&lt;/code&gt; to check the additional information available.&lt;/p&gt;
&lt;p&gt;We can take advantage of this to extract the estimated parameter value and the corresponding log-likelihood:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rate_fit_R$estimate
##      rate 
## 0.9705356
rate_fit_R$loglik
## [1] -25.74768&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, with &lt;code&gt;SciPy&lt;/code&gt; in &lt;code&gt;Python&lt;/code&gt; (using the same data):&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import scipy.stats as stats

sample_data = r.exp_samples

rate_fit_py = stats.expon.fit(data = sample_data, floc = 0)
rate = (rate_fit_py[1])**-1

print(rate)
## 0.9705355729681481&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though we did not specify MLE as a method, the &lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html&#34;&gt;online documentation&lt;/a&gt; indicates this is what the function uses.&lt;/p&gt;
&lt;p&gt;We can also calculate the log-likelihood associated with this estimate using &lt;code&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np

np.sum(np.log(stats.expon.pdf(x = sample_data, 
                              scale = rate_fit_py[1])))
## -25.747680569393435&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve shown that values obtained from &lt;code&gt;Python&lt;/code&gt; match those from &lt;code&gt;R&lt;/code&gt;, so (as usual) both approaches will work out.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;method&lt;/code&gt; argument in &lt;code&gt;R&lt;/code&gt;’s &lt;code&gt;fitdistrplus::fitdist()&lt;/code&gt; function also accepts &lt;code&gt;mme&lt;/code&gt; (moment matching estimation) and &lt;code&gt;qme&lt;/code&gt; (quantile matching estimation), but remember that MLE is the default. One useful feature of MLE, is that (with sufficient data), parameter estimates can be approximated as normally distributed, with the covariance matrix (for all of the parameters being estimated) equal to the inverse of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hessian_matrix&#34;&gt;Hessian matrix&lt;/a&gt; of the likelihood function.&lt;/p&gt;
&lt;p&gt;However, MLE is primarily used as a point estimate solution and the information contained in a single value will always be limited. Likelihoods will not necessarily be symmetrically dispersed around the point of maximum likelihood. We may be interested in the full distribution of credible parameter values, so that we can perform sensitivity analyses and understand the possible outcomes or optimal decisions associated with particular credible intervals. See below for a proposed approach for overcoming these limitations.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations-or-how-to-do-better-with-bayesian-methods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Limitations (or ‘How to do better with Bayesian methods’)&lt;/h3&gt;
&lt;p&gt;An intuitive method for quantifying this &lt;em&gt;epistemic&lt;/em&gt; (statistical) uncertainty in parameter estimation is Bayesian inference. This removes requirements for a sufficient sample size, while providing more information (a full &lt;em&gt;posterior&lt;/em&gt; distribution) of credible values for each parameter. If multiple parameters are being simultaneously estimated, then the posterior distribution will be a joint probabilistic model of all parameters, accounting for any inter-dependencies too. Finally, it also provides the opportunity to build in prior knowledge, which we may have available, before evaluating the data.&lt;/p&gt;
&lt;p&gt;Take it from &lt;code&gt;Stan&lt;/code&gt; himself:
&lt;img src=&#34;https://media.giphy.com/media/3o6Zt7NrhNRgdFkBeU/source.gif&#34; alt=&#34;Stan’s advice&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Returning to the challenge of estimating the rate parameter for an exponential model, based on the same 25 observations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(exp_samples)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.1061  0.4361  0.7552  1.0304  1.2296  4.4239&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now consider a Bayesian approach, by writing a &lt;code&gt;Stan&lt;/code&gt; file that describes this exponential model:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {

  int &amp;lt;lower = 0&amp;gt; N; // Defining the number of observations
  vector &amp;lt;lower = 0&amp;gt; [N] samples; // A vector containing the observations
  
}

parameters {
  
  // The (unobserved) model parameter that we want to estimate
  real &amp;lt;lower = 0&amp;gt; rate;

}

model {

  // An exponential model, which we are proposing to describe the data
  samples ~ exponential(rate);
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with &lt;a href=&#34;https://www.allyourbayes.com/post/bayesian-regression-models-with-stan/&#34;&gt;previous examples&lt;/a&gt; on this blog, data can be pre-processed, and results can be extracted using the &lt;code&gt;rstan&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)

exp_posterior_samples &amp;lt;- sampling(object = exp_model,
                                  data = list(N = n_samples, 
                                              samples = exp_samples),
                                  seed = 1008)
## 
## SAMPLING FOR MODEL &amp;#39;5328d75314acb3313bc7fa418eeb08c2&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.037 seconds (Warm-up)
## Chain 1:                0.034 seconds (Sampling)
## Chain 1:                0.071 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5328d75314acb3313bc7fa418eeb08c2&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.048 seconds (Warm-up)
## Chain 2:                0.029 seconds (Sampling)
## Chain 2:                0.077 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5328d75314acb3313bc7fa418eeb08c2&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.04 seconds (Warm-up)
## Chain 3:                0.032 seconds (Sampling)
## Chain 3:                0.072 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;5328d75314acb3313bc7fa418eeb08c2&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.026 seconds (Warm-up)
## Chain 4:                0.024 seconds (Sampling)
## Chain 4:                0.05 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We have not specified a prior model for the rate parameter. &lt;code&gt;Stan&lt;/code&gt; responds to this by setting what is known as an &lt;em&gt;improper&lt;/em&gt; prior (a uniform distribution bounded only by any upper and lower limits that were listed when the parameter was declared). For real-world problems, there are many reasons to avoid uniform priors. Partly because they are no longer ‘non-informative’ when there are transformations, such as in generalised linear models, and partly because there will always be some prior information to help direct you towards more credible outcomes. However, this data has been introduced without any context and by using uniform priors, we should be able to recover the same maximum likelihood estimate as the non-Bayesian approaches above.&lt;/p&gt;
&lt;p&gt;Extracting the results from our model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmcmc)

extracted_samples &amp;lt;- ggs(S = exp_posterior_samples)

head(x = extracted_samples, n = 5)
## # A tibble: 5 x 4
##   Iteration Chain Parameter value
##       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;     &amp;lt;dbl&amp;gt;
## 1         1     1 rate      0.614
## 2         2     1 rate      0.710
## 3         3     1 rate      0.783
## 4         4     1 rate      1.00 
## 5         5     1 rate      1.16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use this data to visualise the uncertainty in our estimate of the rate parameter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = extracted_samples %&amp;gt;% 
         dplyr::filter(Parameter == &amp;#39;rate&amp;#39;))+
  geom_density(mapping = aes(x = value, 
                             y = ..density..), 
               fill = &amp;#39;purple4&amp;#39;, alpha = 0.2)+
  geom_vline(aes(lty = &amp;#39;MLE solution&amp;#39;, 
                 xintercept = rate_fit_R$estimate))+
  scale_linetype_manual(values = c(2))+
  scale_x_continuous(name = &amp;#39;Rate Parameter&amp;#39;)+
  scale_y_continuous(name = &amp;#39;Posterior Likelihood&amp;#39;)+
  theme_ddf_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-maximum-likelihood-estimation_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use the full posterior distribution to identify the maximum posterior likelihood (which matches the MLE value for this simple example, since we have used an improper prior). However, we can also calculate credible intervals, or the probability of the parameter exceeding any value that may be of interest to us.&lt;/p&gt;
&lt;p&gt;This distribution includes the statistical uncertainty due to the limited sample size. As more data is collected, we generally see a reduction in uncertainty. Based on a similar principle, if we had also have included some information in the form of a prior model (even if it was only weakly informative), this would also serve to reduce this uncertainty.&lt;/p&gt;
&lt;p&gt;Finally, we can also sample from the posterior distribution to plot predictions on a more meaningful outcome scale (where each green line represents an exponential model associated with a single sample from the posterior distribution of the rate parameter):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-28-maximum-likelihood-estimation_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Logistic Regression with Stan</title>
      <link>/post/2020-02-14-bayesian-logistic-regression-with-stan/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-14-bayesian-logistic-regression-with-stan/</guid>
      <description>


&lt;div id=&#34;tldr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TLDR&lt;/h3&gt;
&lt;p&gt;Logistic regression is a popular machine learning model. One application of it in an engineering context is quantifying the effectiveness of inspection technologies at detecting damage. This post describes the additional information provided by a Bayesian application of logistic regression (and how it can be implemented using the &lt;code&gt;Stan&lt;/code&gt; probabilistic programming language). Finally, I’ve also included some recommendations for making sense of priors.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;introductions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introductions&lt;/h3&gt;
&lt;p&gt;So there are a couple of key topics discussed here: Logistic Regression, and Bayesian Statistics. Before jumping straight into the example application, I’ve provided some &lt;strong&gt;very&lt;/strong&gt; brief introductions below.&lt;/p&gt;
&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bayesian Inference&lt;/h4&gt;
&lt;p&gt;At a very high level, Bayesian models quantify (aleatory and epistemic) uncertainty, so that our predictions and decisions take into account the ways in which our knowledge is limited or imperfect. We specify a statistical model, and identify probabilistic estimates for the parameters using a family of sampling algorithms known as Markov Chain Monte Carlo (MCMC). My preferred software for writing a fitting Bayesian models is &lt;a href=&#34;https://mc-stan.org/&#34;&gt;&lt;code&gt;Stan&lt;/code&gt;&lt;/a&gt;. If you are not yet familiar with Bayesian statistics, then I imagine you won’t be fully satisfied with that 3 sentence summary, so I will put together a separate post on the merits and challenges of applied Bayesian inference, which will include much more detail.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Logistic Regression&lt;/h4&gt;
&lt;p&gt;Logistic regression is used to estimate the probability of a binary outcome, such as &lt;em&gt;Pass&lt;/em&gt; or &lt;em&gt;Fail&lt;/em&gt; (though it can be extended for &lt;code&gt;&amp;gt; 2&lt;/code&gt; outcomes). This is achieved by transforming a standard regression using the logit function, shown below. The term in the brackets may be familiar to gamblers as it is how odds are calculated from probabilities. You may see &lt;em&gt;logit&lt;/em&gt; and &lt;em&gt;log-odds&lt;/em&gt; used exchangeably for this reason.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Logit (x) = \log\Bigg({\frac{x}{1 - x}}\Bigg)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since the logit function transformed data &lt;em&gt;from&lt;/em&gt; a probability scale, the inverse logit function transforms data &lt;em&gt;to&lt;/em&gt; a probability scale. Therefore, as shown in the below plot, it’s values range from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt;, and this feature is very useful when we are interested the probability of &lt;em&gt;Pass&lt;/em&gt;/&lt;em&gt;Fail&lt;/em&gt; type outcomes.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Inverse\;Logit (x) = \frac{1}{1 + \exp(-x)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-14-bayesian-logistic-regression-with-stan_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before moving on, some terminology that you may find when reading about logistic regression elsewhere:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When a linear regression is combined with a re-scaling function such as this, it is known as a Generalised Linear Model (&lt;strong&gt;GLM&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;The re-scaling (in this case, the logit) function is known as a &lt;strong&gt;link function&lt;/strong&gt; in this context.&lt;/li&gt;
&lt;li&gt;Logistic regression is a &lt;strong&gt;Bernoulli-Logit GLM&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You may be familiar with libraries that automate the fitting of logistic regression models, either in &lt;code&gt;Python&lt;/code&gt; (via &lt;code&gt;sklearn&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X = dataset[&amp;#39;input_variables&amp;#39;], y = dataset[&amp;#39;predictions&amp;#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…or in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_fit &amp;lt;- glm(formula = preditions ~ input_variables,
                 data = dataset, family = binomial(link = &amp;#39;logit&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-application-probability-of-detection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example Application: Probability of Detection&lt;/h3&gt;
&lt;p&gt;To demonstrate how a Bayesian logistic regression model can be fit (and utilised), I’ve included an example from one of my papers. Engineers make use of data from inspections to understand the condition of structures. Modern inspection methods, whether remote, autonomous or manual application of sensor technologies, are very good. They are generally evaluated in terms of the accuracy and reliability with which they size damage. Engineers never receive perfect information from an inspection, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a crack of &lt;strong&gt;exact&lt;/strong&gt; length &lt;code&gt;30 mm&lt;/code&gt; and &lt;strong&gt;exact&lt;/strong&gt; depth &lt;code&gt;5 mm&lt;/code&gt; at this &lt;strong&gt;exact&lt;/strong&gt; location, or&lt;/li&gt;
&lt;li&gt;There is &lt;strong&gt;definitely&lt;/strong&gt; no damage at this location.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For various reasons, the information we receive from inspections is imperfect and this is something that engineers need to deal with. As a result, providers of inspection services are requested to provide some measure of how good their product is. This typically includes some measure of how accurately damage is sized and how reliable an outcome (detection or no detection) is.&lt;/p&gt;
&lt;p&gt;This example will consider trials of an inspection tool looking for damage of varying size, to fit a model that will predict the probability of detection for any size of damage. Since various forms of damage can initiate in structures, each requiring inspection methods that are suitable, let’s avoid ambiguity and imagine we are only looking for cracks.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media.giphy.com/media/42wQXwITfQbDGKqUP7/giphy.gif&#34; alt=&#34;Detecting Damage: Never 100% Reliable&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Detecting Damage: Never 100% Reliable&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;test-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Test Data&lt;/h4&gt;
&lt;p&gt;For the purposes of this example we will simulate some data. Let’s imagine we have introduced some cracks (of known size) into some test specimens and then arranged for some blind trials to test whether an inspection technology is able to detect them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1008)

N &amp;lt;- 30; lower &amp;lt;- 0; upper &amp;lt;- 10; alpha_true &amp;lt;- -1; beta_true &amp;lt;- 1

depth &amp;lt;- runif(n = N, min = lower, max = upper)

PoD_1D &amp;lt;- function(depth, alpha_1D, beta_1D){
  PoD &amp;lt;- exp(alpha_1D + beta_1D * log(depth)) / (1 + exp(alpha_1D + beta_1D * log(depth)))
  return (PoD)
}

pod_df &amp;lt;- data.frame(depth = depth, det = double(length = N))

for (i in seq(from = 1, to = nrow(pod_df), by = 1)) {
  
  pod_df$det[i] = rbernoulli(n = 1, 
                             p = PoD_1D(depth = pod_df$depth[i], 
                                       alpha_1D = alpha_true, 
                                       beta_1D = beta_true))
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code is used to create 30 crack sizes (depths) between 0 and 10 mm. We then use a log-odds model to back calculate a probability of detection for each. This is based on some fixed values for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. In a real trial, these would not be known, but since we are inventing the data we can see how successful our model ends up being in estimating these values.&lt;/p&gt;
&lt;p&gt;The below plot shows the size of each crack, and whether or not it was detected (in our simulation). The smallest crack that was detected was 2.22 mm deep, and the largest undetected crack was 5.69 mm deep. Even so, it’s already clear that larger cracks are more likely to be detected than smaller cracks, though that’s just about all we can say at this stage.&lt;/p&gt;
&lt;p&gt;After fitting our model, we will be able to predict the probability of detection for a crack of any size.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-14-bayesian-logistic-regression-with-stan_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Stan&lt;/code&gt; is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Probabilistic_programming&#34;&gt;probabilistic programming language&lt;/a&gt;. In a future post I will explain why it has been my preferred software for statistical inference throughout my PhD.&lt;/p&gt;
&lt;p&gt;The below is a simple &lt;code&gt;Stan&lt;/code&gt; program to fit a Bayesian Probability of Detection (PoD) model:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {

  int &amp;lt;lower = 0&amp;gt; N; // Defining the number of defects in the test dataset
  int &amp;lt;lower = 0, upper = 1&amp;gt; det [N]; // A variable that describes whether each defect was detected [1]or not [0]
  vector &amp;lt;lower = 0&amp;gt; [N] depth; // A variable that describes the corresponding depth of each defect
  
  int &amp;lt;lower = 0&amp;gt; K; // Defining the number of probabilistic predictions required from the model
  vector &amp;lt;lower = 0&amp;gt; [K] depth_pred;
  
}

parameters {
  
  // The (unobserved) model parameters that we want to recover
  real alpha;
  real beta;
  
}

model {

  // A logistic regression model relating the defect depth to whether it will be detected
  det ~ bernoulli_logit(alpha + beta * log(depth));
  
  // Prior models for the unobserved parameters
  alpha ~ normal(0, 1);
  beta ~ normal(1, 1);

}

generated quantities {
  
  // Using the fitted model for probabilistic predicition
  // K posterior predictive distributions will be estimated for a corresponding crack depth
  vector [K] postpred_pr;
  
  for (k in 1:K) {
    
    postpred_pr[k] = inv_logit(alpha + beta * log(depth_pred[k]));
    
  }
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;generated quantities&lt;/code&gt; block will be used to make predictions for the &lt;code&gt;K&lt;/code&gt; values of &lt;code&gt;depth_pred&lt;/code&gt; that we provide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;K &amp;lt;- 50; depth_pred &amp;lt;- seq(from = lower, to = upper, length.out = K)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code generates 50 evenly spaced values, which we will eventually combine in a plot. In some instances we may have specific values that we want to generate probabilistic predictions for, and this can be achieved in the same way.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fitting the model&lt;/h4&gt;
&lt;p&gt;Data can be pre-processed in any language for which a &lt;code&gt;Stan&lt;/code&gt; interface has been developed. This includes, &lt;code&gt;R&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, and &lt;code&gt;Julia&lt;/code&gt;. In this example we will use &lt;code&gt;R&lt;/code&gt; and the accompanying package, &lt;code&gt;rstan&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Our &lt;code&gt;Stan&lt;/code&gt; model is expecting data for three variables: &lt;strong&gt;N&lt;/strong&gt;, &lt;strong&gt;det&lt;/strong&gt;, &lt;strong&gt;depth&lt;/strong&gt;, &lt;strong&gt;K&lt;/strong&gt; and &lt;strong&gt;depth_pred&lt;/strong&gt; and &lt;code&gt;rstan&lt;/code&gt; requires this in the form of a list.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Results&lt;/h4&gt;
&lt;p&gt;Once we have our data, and are happy with our model, we can set off the Markov chains. There are plenty of opportunities to control the way that the &lt;code&gt;Stan&lt;/code&gt; algorithm will run, but I won’t include that here, rather we will mostly stick with the default arguments in &lt;code&gt;rstan&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)

PoD_samples &amp;lt;- sampling(object = PoD_model, 
                        data = list(N = N, det = pod_df$det, depth = pod_df$depth,
                                    K = K, depth_pred = depth_pred), 
                        seed = 1008)
## 
## SAMPLING FOR MODEL &amp;#39;7e5f8dcf245c90341e1fcbe2d195277e&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.083 seconds (Warm-up)
## Chain 1:                0.077 seconds (Sampling)
## Chain 1:                0.16 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;7e5f8dcf245c90341e1fcbe2d195277e&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.081 seconds (Warm-up)
## Chain 2:                0.096 seconds (Sampling)
## Chain 2:                0.177 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;7e5f8dcf245c90341e1fcbe2d195277e&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.084 seconds (Warm-up)
## Chain 3:                0.074 seconds (Sampling)
## Chain 3:                0.158 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;7e5f8dcf245c90341e1fcbe2d195277e&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.076 seconds (Warm-up)
## Chain 4:                0.074 seconds (Sampling)
## Chain 4:                0.15 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:I’ve not included any detail here on the checks we need to do on our samples. There are some common challenges associated with MCMC methods, each with plenty of associated guidance on how to diagnose and resolve them. For now, let’s assume everything has gone to plan.&lt;/p&gt;
&lt;p&gt;Now, there are a few options for extracting samples from a stanfit object such as &lt;code&gt;PoD_samples&lt;/code&gt;, including &lt;code&gt;rstan::extract()&lt;/code&gt;. However, these usually require a little post-processing to get them into a tidy format - no big deal, but a hassle I’d rather avoid. That’s why I like to use the &lt;code&gt;ggmcmc&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/ggmcmc/&#34;&gt;package&lt;/a&gt;, which we can use to create a data frame that specifies the iteration, parameter value and chain associated with each data point:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmcmc)

PoD_extracted_samples &amp;lt;- ggs(S = PoD_samples)

head(x = PoD_extracted_samples, n = 5)
## # A tibble: 5 x 4
##   Iteration Chain Parameter   value
##       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;       &amp;lt;dbl&amp;gt;
## 1         1     1 alpha      0.0975
## 2         2     1 alpha     -0.498 
## 3         3     1 alpha     -1.12  
## 4         4     1 alpha     -1.36  
## 5         5     1 alpha     -1.54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have sampled from a 2-dimensional posterior distribution of the unobserved parameters in the model: &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Below is a density plot of their corresponding marginal distributions based on the &lt;code&gt;1000&lt;/code&gt; samples collected from each of the &lt;code&gt;4&lt;/code&gt; Markov chains that have been run.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-14-bayesian-logistic-regression-with-stan_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So our estimates are beginning to converge on the values that were used to generate the data, but this plot also shows that there is still plenty of uncertainty in the results. Unlike many alternative approaches, Bayesian models account for the statistical uncertainty associated with our limited dataset - remember that we are estimating these values from 30 trials. These results describe the possible values of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; in our model that are consistent with the limited available evidence. If more data was available, we could expect the uncertainty in our results to decrease. I think there are some great reasons to keep track of this statistical (sometimes called &lt;em&gt;epistemic&lt;/em&gt;) uncertainty - a primary example being that we should be interested in how confident our predictive models are in their own results!
…but I’ll leave it at that for now, and try to stay on topic.&lt;/p&gt;
&lt;p&gt;How do we know what do these estimates of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; mean for the PoD (what we are ultimately interested in)?
We can check this using the posterior predictive distributions that we have (thanks to the &lt;code&gt;generated quantities&lt;/code&gt; block of the &lt;code&gt;Stan&lt;/code&gt; program).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-14-bayesian-logistic-regression-with-stan_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One thing to note from these results is that the model is able to make much more confident predictions for larger crack sizes. The increased uncertainty associated with shallow cracks reflects the lack of data available in this region - this could be useful information for a decision maker!&lt;/p&gt;
&lt;p&gt;There are only 3 trials in our dataset considering cracks shallower than 3 mm (and only 1 for crack depths &lt;code&gt;&amp;lt; 2&lt;/code&gt; mm). If we needed to make predictions for shallow cracks, this analysis could be extended to quantify the value of future tests in this region.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thought-where-did-those-priors-come-from-and-are-they-any-good&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Final Thought: Where Did Those Priors Come From and Are They Any Good?&lt;/h4&gt;
&lt;p&gt;There are many approaches for specifying prior models in Bayesian statistics. &lt;em&gt;Weakly informative&lt;/em&gt; and &lt;em&gt;MaxEnt&lt;/em&gt; priors are advocated by various authors. Unfortunately, &lt;em&gt;Flat Priors&lt;/em&gt; are sometimes proposed too, particularly (but not exclusively) in older books. A flat prior is a wide distribution - in the extreme this would be a uniform distribution across all real numbers, but in practice distribution functions with very large variance parameters are sometimes used. In either case, a very large range prior of credible outcomes for our parameters is introduced the model. This may sound innocent enough, and in many cases could be harmless.&lt;/p&gt;
&lt;p&gt;Flat priors have the appeal of describing a state of complete uncertainty, which we may believe we are in before seeing any data - but is this really the case?&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media.giphy.com/media/UgM7H8OEmf4mQ/giphy.gif&#34; alt=&#34;Prior Expectations: Can We Do Better?&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Prior Expectations: Can We Do Better?&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Suppose you are using Bayesian methods to model the speed of some athletes. Even before seeing any data, there is some information that we can build into the model. For instance, we can discount negative speeds. We also wouldn’t need to know anything about the athletes to know that they would not be travelling faster than the speed of light. This may sound facetious, but flat priors are implying that we should treat all outcomes as equally likely. In fact, there are some cases where flat priors cause models to require large amounts of data to make good predictions (meaning we are failing to take advantage of Bayesian statistics ability to work with limited data).&lt;/p&gt;
&lt;p&gt;In this example, we would probably just want to constrain outcomes to the range of metres per second, but the amount of information we choose to include is ultimately a modelling choice. Another helpful feature of Bayesian models is that the priors are part of the model, and so must be made explicit - fully visible and ready to be scrutinised.&lt;/p&gt;
&lt;p&gt;A common challenge, which was evident in the above PoD example, is lacking an intuitive understanding of the meaning of our model parameters. Here &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; required prior models, but I don’t think there is an obvious way to relate their values to the result we were interested in. They are linear regression parameters on a log-odds scale, but this is then transformed into a probability scale using the logit function.&lt;/p&gt;
&lt;p&gt;This problem can be addressed using a process known as &lt;strong&gt;Prior Predictive Simulation&lt;/strong&gt;, which I was first introduced to in &lt;a href=&#34;https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919&#34;&gt;Richard McElreath’s fantastic book&lt;/a&gt;. This involves evaluating the predictions that our model would make, based only on the information in our priors. Relating our predictions to our parameters provides a clearer understanding of the implications of our priors.&lt;/p&gt;
&lt;p&gt;Back to our PoD parameters - both &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; can take positive or negative values, but I could not immediately tell you a sensible range for them. Based on our lack of intuition it may be tempting to use a variance for both, right? Well, before making that decision, we can always simulate some predictions from these priors. The below code is creating a data frame of prior predictions for the PoD (&lt;code&gt;PoD_pr&lt;/code&gt;) for many possible crack sizes.
&lt;span class=&#34;math display&#34;&gt;\[
\alpha \sim N(\mu_{\alpha}, \sigma_{\alpha})
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\beta \sim N(\mu_{\beta}, \sigma_{\beta})
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(from = min_depth, to = max_depth, length.out = N_samples)
prPrSim_df &amp;lt;- data.frame(depth = x)

for (i in seq(from = 1, to = nrow(prPrSim_df), by = 1)) {
  
  alpha = rnorm(n = N_samples, mean = mu_alpha, sd = sigma_alpha)
  beta = rnorm(n = N_samples, mean = mu_beta, sd = sigma_beta)
  prPrSim_df$PoD_pr[i] &amp;lt;- exp(alpha + beta * log(prPrSim_df$depth[i]))/(1 + exp(alpha + beta * log(prPrSim_df$depth[i])))

}

head(prPrSim_df)
##        depth       PoD_pr
## 1 0.00000000 0.000000e+00
## 2 0.01001001 4.085619e-05
## 3 0.02002002 5.966123e-02
## 4 0.03003003 4.766970e-05
## 5 0.04004004 4.532144e-03
## 6 0.05005005 2.997338e-03&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can visualise the information contained within our priors for a couple of different cases.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-14-bayesian-logistic-regression-with-stan_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our wide, supposedly &lt;em&gt;non&lt;/em&gt;-informative priors result in some pretty useless predictions. I’ve suggested some more sensible priors that suggest that larger cracks are more likely to be detected than small cracks, without overly constraining our outcome (see that there is still prior credible that very small cracks are detected reliably and that very large cracks are often missed).&lt;/p&gt;
&lt;p&gt;Why did our predictions end up looking like this?&lt;/p&gt;
&lt;p&gt;Borrowing from McElreath’s explanation, it’s because &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; are linear regression parameters on a log-odds (logit) scale. Since we are estimating a PoD we end up transforming out predictions onto a probability scale. Flat priors for our parameters imply that extreme values of log-odds are credible. All that prior credibility of values &lt;code&gt;&amp;lt; - 3&lt;/code&gt; and &lt;code&gt;&amp;gt; 3&lt;/code&gt; ends up getting concentrated at probabilities near &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;. I think this is a really good example of flat priors containing a lot more information than they appear to.&lt;/p&gt;
&lt;p&gt;I’ll end by directing you towards some additional (generally non-technical) discussion of choosing priors, written by the &lt;code&gt;Stan&lt;/code&gt; development team &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;(link)&lt;/a&gt;. It provides a definition of &lt;em&gt;weakly informative priors&lt;/em&gt;, some words of warning against &lt;em&gt;flat priors&lt;/em&gt; and more general detail than this humble footnote.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-14-bayesian-logistic-regression-with-stan_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
