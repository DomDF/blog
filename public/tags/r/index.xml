<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on All Your Bayes</title>
    <link>/tags/r/</link>
    <description>Recent content in R on All Your Bayes</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-uk</language>
    <lastBuildDate>Tue, 24 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Why Go Bayesian?</title>
      <link>/post/why-go-bayesian/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/why-go-bayesian/</guid>
      <description>


&lt;div id=&#34;tldr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TLDR&lt;/h3&gt;
&lt;p&gt;This post is intended to be a high-level discussion of the merits and challenges of applied Bayesian statistics. It is intended to help the reader answer: &lt;em&gt;Is it worth me learning Bayesian statistics?&lt;/em&gt; or &lt;em&gt;Should I look into using Bayesian statistics in my project?&lt;/em&gt; Maths, code and technical details have all been left out.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;bayes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bayes&lt;/h4&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media.giphy.com/media/TJBbXQooivUNq/giphy.gif&#34; alt=&#34;Bayes&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Bayes&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Firstly, Bayesian…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Inference&lt;/li&gt;
&lt;li&gt;Modelling&lt;/li&gt;
&lt;li&gt;Updating&lt;/li&gt;
&lt;li&gt;Data Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;…can be considered the same thing (certainly for the purposes of this post): &lt;strong&gt;the application of Bayes theorem to quantify uncertainty&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So Bayesian statistics may be of interest to you if you are dealing with a problem associated with uncertainty - either due to some underlying variability, or due to limitations of your data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-a-bayesian-approach-provide&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What does a Bayesian Approach Provide?&lt;/h3&gt;
&lt;p&gt;Bayesian statistics is not the only way to account for uncertainty in calculations. The below points describe what Bayesian approach offers that others don’t. Note that I am only really discussing methods involving probability here, though &lt;a href=&#34;https://www.springer.com/gp/book/9783540402947&#34;&gt;alternative approaches are available&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;interpretation-of-results&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Interpretation of Results&lt;/h4&gt;
&lt;p&gt;The outcome of a Bayesian model is a posterior distribution. This describes the joint uncertainty in all the parameters we are trying to estimate. A posterior predictive distribution uses this information to describe uncertainty in a prediction that this model would make for some new input data. By comparison, alternative (frequentist) statistical analysis typically describes uncertainty in predictions using confidence intervals, which are widely used but easy to misinterpret.&lt;/p&gt;
&lt;p&gt;Confidence intervals are calculated so that they will contain the &lt;strong&gt;true&lt;/strong&gt; value of whatever you are trying to predict with some desired frequency. They provide no information (in the absence of additional assumptions) on how credible various possible results are. The Bayesian equivalent (sometimes called credible intervals) can be drawn anywhere on a predictive distribution. &lt;a href=&#34;https://mitpress.mit.edu/books/introduction-statistical-decision-theory&#34;&gt;Pratt, Raiffa and Schlaiffer&lt;/a&gt; provide the following summary of the two approaches:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Imagine the plight of the manager who exclaims, ‘I understand [does he?] the meaning that the demand for XYZ will lie in the interval 973 to 1374 with confidence .90. However, I am particularly interested in the interval 1300 to 1500. What confidence can I place on that interval?’&lt;/em&gt;
&lt;em&gt;Unfortunately, this question cannot be answered. Of course, however, it is possible to give a posterior probability to that particular interval - or any other - based on the sample data and on a codification of the manager’s prior judgements.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And a more succinct description of the same view is presented in &lt;a href=&#34;https://www.weirdfishes.blog/&#34;&gt;Dan Ovando’s fishery statistics blog&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayesian credible intervals mean what we’d like Frequentist confidence intervals to mean.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seamless-integration-with-decision-analysis&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Seamless Integration with Decision Analysis&lt;/h4&gt;
&lt;p&gt;Following on from the previous point, an analysis that directly describes the probability of any outcome is fully compatible with a decision analysis. After completing a Bayesian analysis, identifying the optimal strategy implied by your model becomes simpler and more understandable.&lt;/p&gt;
&lt;p&gt;As stated in &lt;a href=&#34;https://www.springer.com/gp/book/9780387960982&#34;&gt;James Berger’s (quite theoretical) book on Bayesian statistics&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayesian analysis and decision theory go rather naturally together, partly because of their common goal of utilizing non-experimental sources of information, and partly because of deep theoretical ties.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flexible-modelling&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Flexible Modelling&lt;/h4&gt;
&lt;p&gt;So this one is based on a point made in &lt;a href=&#34;https://uk.sagepub.com/en-gb/eur/a-student%E2%80%99s-guide-to-bayesian-statistics/book245409&#34;&gt;Ben Lambert’s book on Bayesian statistics&lt;/a&gt;. It is regarding how modern Bayesian statistics is achieved in practice The computational methods require some effort to pick up, especially if you do not have experience with programming (though Ben Lambert’s book gives a nice introduction to &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;). However, they can be extended to larger and more complex models in an intuitive way.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media.giphy.com/media/WiyczarN2XMm4/giphy.gif&#34; alt=&#34;Some Compelling Arguments&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Some Compelling Arguments&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;challenges-difficulties&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Challenges &amp;amp; Difficulties&lt;/h3&gt;
&lt;p&gt;So why would anyone ever &lt;em&gt;not&lt;/em&gt; use Bayesian models when making predictions?&lt;/p&gt;
&lt;div id=&#34;subjectivity&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Subjectivity&lt;/h4&gt;
&lt;p&gt;Perhaps the most common criticism of Bayesian statistics is the requirement for prior models. An initial estimate of uncertainty is a term in Bayes’ theorem - but how can you estimate the extent of variability before you see it in your data? This will surely be completely subjective, so the results will vary depending on who is doing the analysis. This, understandably, doesn’t seem right with a lot of casual enquirers.&lt;/p&gt;
&lt;p&gt;A common response to this accusation is that subjectivity is not an exclusive feature of Bayesian analysis (how about the whole structure of the model you are trying to fit, by any means?) &lt;em&gt;but&lt;/em&gt; at least Bayesians are required to be explicit about it. Priors are part of the model with no-where to hide (in the code or the reporting) and so they are open to criticism. This point is discussed in &lt;strong&gt;much&lt;/strong&gt; more detail in this paper from &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/gelman_hennig_full_discussion.pdf&#34;&gt;Colombia University&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Priors can contain, as much or as little, information as desired. However, even in instances where you may feel you don’t have any upfront knowledge of a problem, they represent a valuable opportunity for introducing regularisation (which protects against bad predictions due to overfitting). This idea is discussed in detail in &lt;a href=&#34;https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919&#34;&gt;Richard McElreath’s textbook&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computational-requirements&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Computational Requirements&lt;/h4&gt;
&lt;p&gt;In practice, statisticians estimate Bayesian posterior distributions using Markov Chain Monte Carlo (MCMC) sampling algorithms. This approach is slower, more complicated and less informative than standard, independent Monte Carlo sampling. The models that I have worked with during my PhD have taken several hours to finish sampling from. Following this, there are checks that need to be completed as there are plenty of things that can go wrong with MCMC.&lt;/p&gt;
&lt;p&gt;My background is in mechanical and civil engineering. In my discussions with engineering researchers at conferences I have often been told that the errors and complications they encountered when using MCMC methods had made them believe that Bayesian statistics wasn’t for them. These are challenges that I imagine everyone who has attempted modern Bayesian statistics will have encountered and resolving them requires a deeper understanding of your model. Both domain-specific and statistical knowledge is required to help ensure the model you are trying to fit is justified. In addition some programming &lt;em&gt;tricks&lt;/em&gt; like reparameterisation can be of great help to your software, which sometimes needs equivalent, but easier to interpret instructions.&lt;/p&gt;
&lt;p&gt;With all that in mind, when would this ever be worthwhile?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;Regardless of whether you believe we exist in a deterministic universe or not, you will never have perfect state of knowledge describing your problem: uncertainty exists, so are you sure that your approach is sensible or safe?&lt;/p&gt;
&lt;p&gt;I believe that Bayesian statistics is actually well suited to traditional engineering problems, as we are concerned with managing risk when confronted with small, messy datasets and models with plenty of uncertainty. As suggested in the earlier description of confidence interval, frequentist statistics defines probability based on occurrences of events following a large number of trials or samples. When only limited data is available, Bayesian statistics shines by comparison.&lt;/p&gt;
&lt;p&gt;Very large datasets may contain enough information to precisely estimate parameters in a model using standard machine learning methods, and so it becomes less worthwhile running simulations to characterise variability. But how common are these big data problems in science and engineering? Sometimes large populations of data are better described as many smaller groups, after accounting for key differences between them. Bayesian statistics has a very useful way of managing such problems by structuring models hierarchically. This method allows for &lt;strong&gt;partial pooling of information&lt;/strong&gt; between groups, so that predictions account for the variability and commonality between groups. I will provide a detailed example of this in a future post.&lt;/p&gt;
&lt;p&gt;In conclusion, Bayesian statistics requires (computational and personal) effort to apply. But it provides results that are (usually) more interpretable and closely linked to questions we want to answer. Whether or not these methods are worth learning of course depend on your personal circumstances. I encountered Bayesian statistics during my PhD, and so had plenty of time to read up and I have personally found this to be very rewarding and enjoyable…&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media.giphy.com/media/WPLPEu0GUp41W/giphy.gif&#34; alt=&#34;Boring, isn’t it? Writing, Fitting and Evaluating Bayesian Models All Day….&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Boring, isn’t it? Writing, Fitting and Evaluating Bayesian Models All Day….&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Animating Plots</title>
      <link>/post/animating-plots/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/animating-plots/</guid>
      <description>


&lt;div id=&#34;tldr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TLDR&lt;/h3&gt;
&lt;p&gt;There are many instances where it may be useful to animate graphical representations of data, perhaps to add an additional dimension to a plot. The below example builds a cumulative map of car accidents in the UK using some of the functionality of the &lt;code&gt;gganimate&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/gganimate&#34;&gt;package&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;making-moving-plots-with-gganimate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making Moving Plots with &lt;code&gt;gganimate&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Graphics made using the &lt;code&gt;ggplot2&lt;/code&gt; package are already extremely customisable. They can be further enhanced using some of the &lt;a href=&#34;http://ggplot2-exts.org/gallery/&#34;&gt;extensions that have been developed&lt;/a&gt;. These include providing access to new themes, as well as entirely new functionality.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;gganimate&lt;/code&gt; allows for the animation of existing ggplot graphics. Once installed, we can load both packages (&lt;code&gt;ggplot2&lt;/code&gt; is included as part of the &lt;code&gt;tidyverse&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gganimate); library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The example uses a car accident dataset that I found on &lt;a href=&#34;https://www.kaggle.com/silicon99/dft-accident-data/data#&#34;&gt;Kaggle&lt;/a&gt;. Here are the first few rows of the variables that we’re interested in:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(Accidents_Dec2015 %&amp;gt;% 
       dplyr::select(Date, Longitude, Latitude, Number_of_Casualties, Accident_Severity))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Date Longitude Latitude Number_of_Casualties Accident_Severity
## 1 2015-12-01 -0.155880 51.48959                    1            Slight
## 2 2015-12-01 -0.200271 51.49262                    1            Slight
## 3 2015-12-03 -0.210643 51.49997                    2            Slight
## 4 2015-12-03 -0.156754 51.49293                    1            Slight
## 5 2015-12-03 -0.159124 51.50205                    1            Slight
## 6 2015-12-04 -0.197452 51.49104                    1            Slight&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the coordinates using a map of the UK included in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;UK_coords &amp;lt;- ggplot2::map_data(map = &amp;#39;world&amp;#39;, region = &amp;#39;UK&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before animating we need to create a ggplot that we will work from.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;accidents_plot &amp;lt;- ggplot(data = UK_coords)+
  geom_polygon(mapping = aes(x = long, y = lat, group = group), col = &amp;#39;black&amp;#39;, fill = NA)+
  theme_void(base_size = 12, base_family = &amp;#39;Bahnschrift&amp;#39;)+
  geom_point(data = Accidents_Dec2015 %&amp;gt;% dplyr::filter(grepl(pattern = &amp;#39;Slight&amp;#39;, x = Accident_Severity)), 
             mapping = aes(x = Longitude, y = Latitude, col = as.factor(Accident_Severity), size = Number_of_Casualties), 
             alpha = 0.1)+
  geom_point(data = Accidents_Dec2015 %&amp;gt;% dplyr::filter(grepl(pattern = &amp;#39;Serious&amp;#39;, x = Accident_Severity)), 
             mapping = aes(x = Longitude, y = Latitude, col = as.factor(Accident_Severity), size = Number_of_Casualties), 
             alpha = 0.2)+
  geom_point(data = Accidents_Dec2015 %&amp;gt;% dplyr::filter(grepl(pattern = &amp;#39;Fatal&amp;#39;, x = Accident_Severity)), 
             mapping = aes(x = Longitude, y = Latitude, col = as.factor(Accident_Severity), size = Number_of_Casualties), 
             alpha = 0.4)+
  theme(legend.position = &amp;#39;right&amp;#39;)+
  scale_size_continuous(breaks = c(1, 3, 9))+
  scale_color_manual(values = c(&amp;#39;firebrick&amp;#39;, &amp;#39;forestgreen&amp;#39;, &amp;#39;steelblue&amp;#39;))+
  guides(col = guide_legend(title = element_blank(), ncol = 1),
         size = guide_legend(title = element_text(&amp;#39;Casualties&amp;#39;, size = 10), ncol = 1))

accidents_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-04-animating-plots_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can now add some functions from &lt;code&gt;gganimate&lt;/code&gt;, which will describe how and saving as a new variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gganimate)

accidents_plot &amp;lt;- accidents_plot +
  transition_time(time = Date)+
  shadow_trail()+
  ggtitle(label = &amp;#39;UK Car Accidents in December 2015&amp;#39;, subtitle = &amp;#39;Date : {frame_time}&amp;#39;)+
  labs(caption = &amp;#39;Data from Kaggle: https://www.kaggle.com/silicon99/dft-accident-data/data |  @d73mwf&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;transition_time()&lt;/code&gt; requires specification of a time variable that the plot will display sequentially. As shown above, this animation will transition through values of &lt;code&gt;Date&lt;/code&gt;. There are many more options that allow for animation across different data types in different ways.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;shadow_mark()&lt;/code&gt; has been added to keep accidents from previous dates. Again, there are various methods of showing data from previous states.&lt;/p&gt;
&lt;p&gt;Some of these additional options are detailed &lt;a href=&#34;https://cran.r-project.org/web/packages/gganimate&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this case, &lt;code&gt;transition_length&lt;/code&gt; and &lt;code&gt;state_length&lt;/code&gt; describe the &lt;strong&gt;relative&lt;/strong&gt; amount of time spent displaying the current state, and transitioning to the next state.&lt;/p&gt;
&lt;p&gt;Regardless of the transition function selected, the best way to create the moving plot is to use the &lt;code&gt;animate&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;animate(plot = accidents_plot, fps = 20, duration = 30, end_pause = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;animate()&lt;/code&gt; function requires us to specify the plot (to be animated), but includes many additional arguments not all of which are detailed above.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;fps&lt;/code&gt; is the frames per second, &lt;code&gt;duration&lt;/code&gt; is the length of the animation (in seconds), and &lt;code&gt;end_pause&lt;/code&gt; is the length of time that the final frame is held for (in number of frames.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-animation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Final Animation&lt;/h3&gt;
&lt;p&gt;I tried a few alternatives here. In this case each state has quite a few points and so I wanted it to be held for a reasonable amount of time. The trade-off here is the number of frames (and associated processing time and file size). The below allocates approximately &lt;code&gt;1&lt;/code&gt; second per day and results in a total of 600 frames for the animation. On my (ageing) laptop this required approximately 4 minutes to render.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2019-06-04-animating-plots_files/car_acc_Dec2015.gif&#34; alt=&#34;Animated Map&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Animated Map&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;thoughts-on-the-animation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thoughts on the Animation&lt;/h3&gt;
&lt;p&gt;What does the animation tell us that the stationary plot doesn’t?&lt;/p&gt;
&lt;p&gt;The final frame is pretty much identical, but the transitions will show when the accidents occurred. From viewing the animation there doesn’t appear to be a clear time when accidents were more frequent. We can check this with an additional plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Accidents_Dec2015 %&amp;gt;% 
     dplyr::select(Date, Number_of_Casualties, Accident_Severity) %&amp;gt;% 
     ggplot(mapping = aes(x = Date))+
        geom_bar(stat = &amp;#39;count&amp;#39;, fill = &amp;#39;grey80&amp;#39;)+
        facet_wrap(facets = ~ Accident_Severity, scales = &amp;#39;free&amp;#39;)+
        #ggthemes::theme_tufte(base_size = 12, base_family = &amp;#39;Bahnschrift&amp;#39;)+
        theme_ddf_light()+
        coord_flip()+
        theme(axis.title = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-04-animating-plots_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What isn’t shown in the data is the number of cars that were on the road at the time, so it could be that there were a higher proportion of accidents between the &lt;code&gt;24&lt;/code&gt;th and &lt;code&gt;31&lt;/code&gt;st December - but that will have to remain speculation for now.&lt;/p&gt;
&lt;p&gt;Another slight issue I have with the final animation is the way the points move to the next location. Given the context of the plot, it could be interpreted as the same vehicles having accidents all over the UK. When critically considering this idea it becomes apparent that it cannot be the case (for &lt;em&gt;many&lt;/em&gt; reasons), but there should be as little ambiguity as possible in a plot like this.&lt;/p&gt;
&lt;p&gt;In conclusion, I think the animated plot looks cool, but it is perhaps a little gimmicky for this particular application. The same information is contained in the two static plots in this post. However, I hope this has content serves as a gentle introduction to how the &lt;code&gt;gganimate&lt;/code&gt; package can automate animated graphics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-resources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;p&gt;This example is certainly not exhaustive and there are many additional tweaks available to further customise an animation. I have personally found the below resources to be particularly helpful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The official &lt;a href=&#34;https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html&#34;&gt;beginner’s guide&lt;/a&gt; to &lt;code&gt;gganimate&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A tutorial from the &lt;a href=&#34;https://goodekat.github.io/presentations/2019-isugg-gganimate-spooky/slides.html#1&#34;&gt;ISU Graphics Group&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Wordclouds</title>
      <link>/post/31.05.19-wordcloud/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/31.05.19-wordcloud/</guid>
      <description>


&lt;div id=&#34;tldr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TLDR&lt;/h3&gt;
&lt;p&gt;Wordclouds can be used to produce a neat summary of text and can readily be produced in R. This is a simple example based on a recent conferene paper.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;summarising-the-content-of-a-conference-paper&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summarising the content of a conference paper&lt;/h3&gt;
&lt;p&gt;There is an &lt;a href=&#34;https://cran.r-project.org/web/packages/wordcloud/index.html&#34;&gt;R package dedicated to creating wordclouds&lt;/a&gt;, so I’ve started by loading this, along with the &lt;strong&gt;tidyverse&lt;/strong&gt; (for standard data manipulation) and &lt;strong&gt;tidytext&lt;/strong&gt; (for some help processing the contents of the paper).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud); library(tidyverse); library(tidytext)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The wordcloud package creates a graphic of words that appear in some specified text. The size of the word is proprtional to its frequency in the text
R can read text from a local file, as shown below, or from a website.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can read a text file using &amp;#39;readLines&amp;#39; and we can select a file interactively using &amp;#39;file.choose&amp;#39;
# Both of these are Base R functions
paper &amp;lt;- readLines(file.choose())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ‘paper’ variable is currently as list of individual lines, as we can see when viewing one of its elements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paper[2])
## [1] &amp;quot;Application of MCMC Sampling to Account for Variability and Dependency&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;tidytext&lt;/strong&gt; helps get this into a friendlier format allowing us to count the occirence of each word.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paper_tbl &amp;lt;- as_tibble(paper) %&amp;gt;% 
  tidytext::unnest_tokens(word, value) %&amp;gt;% 
  dplyr::filter(is.na(as.numeric(word))) %&amp;gt;% 
  count(word)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since I expected words like ‘the’ and ‘of’ are likely to feature a lot in the text, I wanted to be able to remove them. I initally used &lt;strong&gt;dplyr&lt;/strong&gt; to set up a variable that would allow me to filter out shorter words, based on some threshold…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;minLength &amp;lt;- 4

paper_tbl &amp;lt;- paper_tbl %&amp;gt;% 
  mutate(check = case_when(nchar(word) &amp;lt; minLength ~ 0,
                           nchar(word) &amp;gt;= minLength ~ 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But then I learnt about &lt;a href=&#34;https://en.wikipedia.org/wiki/Stop_words&#34;&gt;stopwords&lt;/a&gt; and made use of the database that &lt;strong&gt;tidytext&lt;/strong&gt; conveniently provides, before removing them from the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paper_tbl &amp;lt;- paper_tbl %&amp;gt;%  
 anti_join(tidytext::get_stopwords(language = &amp;#39;en&amp;#39;, source = &amp;#39;stopwords-iso&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before sending this directly into the wordcloud function, we can review the current state of the data, either as a table…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(x = paper_tbl %&amp;gt;% 
       arrange(desc(x = n)), n = 10)
## # A tibble: 10 x 2
##    word           n
##    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt;
##  1 model         58
##  2 models        27
##  3 fatigue       25
##  4 data          24
##  5 parameters    24
##  6 bayesian      22
##  7 posterior     18
##  8 crack         17
##  9 growth        14
## 10 priors        12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…or as a simple plot (in either case I’m only interested in the most frequent words for now) …&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(paper_tbl %&amp;gt;% 
         arrange(desc(n)) %&amp;gt;% 
         dplyr::filter(n &amp;gt;= 12))+
  geom_col(mapping = aes(x = word, y = n))+
  theme_minimal()+ theme(axis.text.x = element_text(angle = 90), 
                         axis.title.x = element_blank())+
  labs(y = &amp;#39;count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/31.05.19-Wordcloud_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One thing that is apparent from the above summaries is that we have not dealt with plurals from the data, i.e. ‘model’ and ‘models’ will be treated as two different words, with their own count. I’ve not found a neat way to combine these, but a manual solution with regular expressions (such as &lt;em&gt;grepl&lt;/em&gt;, &lt;em&gt;!grepl&lt;/em&gt;, etc.) would be simple, though not very elegant. I decided to leave plurals as they are.&lt;/p&gt;
&lt;p&gt;Finally, time to ask the wordcloud function to read and plot our data. There are some useful arguments to experiment with here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;min.freq&lt;/strong&gt; and &lt;strong&gt;max.words&lt;/strong&gt; set boundaries for how populated the wordcloud will be&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;random.order&lt;/strong&gt; will put the largest word in the middle if set to FALSE&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rot.per&lt;/strong&gt; is the fraction of words that will be rotated in the graphic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, the words are arranged stochastically somehow, and so for a repeatable graphic we need to specify a seed value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1008) 

wordcloud(words = paper_tbl$word, freq = paper_tbl$n, 
          min.freq = 4, max.words = 100, random.order = FALSE, rot.per = 0.25,
          colors = brewer.pal(n = 8, name = &amp;#39;Paired&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/31.05.19-Wordcloud_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you’re not familiar with the colour palettes, the below line will ask R to display them for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RColorBrewer::display.brewer.all()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, some links to more information regarding the packages introduced here, both of which are available on CRAN:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/wordcloud/index.html&#34;&gt;wordcloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html&#34;&gt;tidytext&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
