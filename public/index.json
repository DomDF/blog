[{"authors":["admin"],"categories":null,"content":"My research interests include Bayesian methods of degradation modelling and it\u0026rsquo;s decision theoretic applications including quantification of the expected value of information. I also work in football analytics.\n","date":1576022400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1576022400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"My research interests include Bayesian methods of degradation modelling and it\u0026rsquo;s decision theoretic applications including quantification of the expected value of information. I also work in football analytics.","tags":null,"title":"Domenic Di Francesco","type":"authors"},{"authors":[],"categories":["Bayesian","Stan","Decision Analysis"],"content":"\rTLDR\rThis post is intended to be a high-level discussion of the merits and challenges of applied Bayesian statistics. It is intended to help the reader answer: Is it worth me learning Bayesian statistics? or Should I look into using Bayesian statistics in my project? Maths, code and technical details have all been left out.\nBayes\rBayes\n\r\r\rIntroduction\rFirstly, Bayesian…\n\rStatistics\rInference\rModelling\rUpdating\rData Analysis\r\r…can be considered the same thing (certainly for the purposes of this post): the application of Bayes theorem to quantify uncertainty.\nSo Bayesian statistics may be of interest to you if you are dealing with a problem associated with uncertainty - either due to some underlying variability, or due to limitations of your data.\n\rWhat does a Bayesian Approach Provide?\rBayesian statistics is not the only way to account for uncertainty in calculations. The below points describe what a Bayesian approach offers, that others don’t. Note that I am only really discussing methods involving probability here, though alternative approaches are available.\nIntuitive Interpretation of Results\rThe outcome of a Bayesian model is a posterior distribution. This describes the joint uncertainty in all the parameters you are trying to estimate. This can be used to describe uncertainty in a prediction for some new input data. By comparison, alternative (frequentist) methods typically describes uncertainty in predictions using confidence intervals, which are widely used but easy to misinterpret.\nConfidence intervals are calculated so that they will contain the true value of whatever you are trying to predict with some desired frequency. They provide no information (in the absence of additional assumptions) on how credible various possible results are. The Bayesian equivalent (sometimes called credible intervals) can be drawn anywhere on a predictive distribution. In Pratt, Raiffa and Schlaiffer’s textbook an example is used to highlight this difference:\nImagine the plight of the manager who exclaims, ‘I understand [does he?] the meaning that the demand for XYZ will lie in the interval 973 to 1374 with confidence .90. However, I am particularly interested in the interval 1300 to 1500. What confidence can I place on that interval?’\rUnfortunately, this question cannot be answered. Of course, however, it is possible to give a posterior probability to that particular interval - or any other - based on the sample data and on a codification of the manager’s prior judgements.\nAnd a more succinct description of the same view from Dan Ovando’s fishery statistics blog:\nBayesian credible intervals mean what we’d like Frequentist confidence intervals to mean.\n\rSeamless Integration with Decision Analysis\rFollowing on from the previous point, an analysis that directly describes the probability of any outcome is fully compatible with a decision analysis. After completing a Bayesian analysis, identifying the optimal strategy implied by your model becomes simpler and more understandable.\nAs stated in James Berger’s (quite theoretical) book on Bayesian statistics:\nBayesian analysis and decision theory go rather naturally together, partly because of their common goal of utilizing non-experimental sources of information, and partly because of deep theoretical ties.\n\rFlexible Modelling\rSo this one is based on a point made in Ben Lambert’s book on Bayesian statistics. It is regarding how modern Bayesian statistics is achieved in practice. The computational methods require some effort to pick up, especially if you do not have experience with programming (though Ben Lambert’s book gives a nice introduction to Stan). However, they can be readily extended to larger and more complex models.\nSome Compelling Arguments\n\r\r\rChallenges \u0026amp; Difficulties\rSo why would anyone ever not use Bayesian models when making predictions?\nSubjectivity\rPerhaps the most common criticism of Bayesian statistics is the requirement for prior models. An initial estimate of uncertainty is a term in Bayes’ theorem - but how can you estimate the extent of variability before you see it in your data? This will surely be completely subjective, so the results will vary depending on who is doing the analysis. This, understandably, doesn’t seem right with a lot of casual enquirers.\nA common response to this accusation is that subjectivity is not an exclusive feature of Bayesian analysis (how about the whole structure of the model you are trying to fit, regardless of your method?) …but at least Bayesians are required to be explicit about it. Priors are part of the model with no-where to hide (in the code or the reporting) and so they are open to criticism. This point is discussed in much more detail in this paper from Colombia University.\nPriors can contain, as much or as little, information as desired. However, even in instances where you may feel you don’t have any upfront knowledge of a problem, they represent a valuable opportunity for introducing regularisation (which protects against bad predictions due to overfitting). This idea is discussed in detail in Richard McElreath’s textbook.\n\rComputational Requirements\rIn practice, statisticians estimate Bayesian posterior distributions using Markov Chain Monte Carlo (MCMC) sampling algorithms. This approach is slower, more complicated and less informative than standard, independent Monte Carlo sampling. The models that I have worked with during my PhD have taken several hours to finish sampling from, but I have met statisticians whose models run for days or even weeks. Following this, there are checks that need to be completed as there are plenty of things that can go wrong with MCMC.\nMy background is in mechanical and civil engineering. In discussions with engineering researchers at conferences I have often been told that the errors and complications they encountered when using MCMC methods had made them believe that Bayesian statistics wasn’t for them. These are challenges that I imagine everyone who has attempted modern Bayesian statistics will have encountered and resolving them can require a deep understanding of your model. Both domain-specific and statistical knowledge is required to help ensure the model you are trying to fit is justified. In addition some programming tricks like reparameterisation can be of great help to your software, which sometimes needs equivalent, but easier to interpret instructions.\nWith all that in mind, when would this ever be worthwhile?\n\r\rConclusions\rRegardless of whether you believe we exist in a deterministic universe or not, you will never have perfect state of knowledge describing your problem: uncertainty exists, so we need a sensible and safe way of accounting for it.\nI believe that Bayesian statistics is actually well suited to traditional engineering problems, which are concerned with managing risk when confronted with small, messy datasets and models with plenty of uncertainty. As suggested in the earlier description of confidence intervals, frequentist statistics defines probability based on occurrences of events following a large number of trials or samples. When only limited data is available, Bayesian statistics can shine by comparison.\nVery large datasets may contain enough information to precisely estimate parameters in a model using standard machine learning methods, and so it becomes less worthwhile running simulations to characterise variability. But how common are these big data problems in science and engineering? Sometimes large populations of data are better described as multiple smaller constituent groups, after accounting for key differences between them. Bayesian statistics has a very useful way of managing such problems by structuring models hierarchically. This method allows for partial pooling of information between groups, so that predictions account for the variability and commonality between groups. I will provide a detailed example of this in a future post.\nIn conclusion, Bayesian statistics requires (computational and personal) effort to apply. But it provides results that are (usually) more interpretable and closely linked to the questions we want to answer. Whether or not these methods are worth learning of course depend on personal circumstances. I encountered Bayesian statistics during my PhD, and so had plenty of time to read up and I’ve found this to be very rewarding and enjoyable…\nBoring, isn’t it? Writing, Fitting and Evaluating Bayesian Models All Day….\n\r\r","date":1585008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585094113,"objectID":"98412b067775d084dca3fc12fd9141a8","permalink":"/post/why-go-bayesian/","publishdate":"2020-03-24T00:00:00Z","relpermalink":"/post/why-go-bayesian/","section":"post","summary":"TLDR\rThis post is intended to be a high-level discussion of the merits and challenges of applied Bayesian statistics. It is intended to help the reader answer: Is it worth me learning Bayesian statistics? or Should I look into using Bayesian statistics in my project? Maths, code and technical details have all been left out.\nBayes\rBayes\n\r\r\rIntroduction\rFirstly, Bayesian…\n\rStatistics\rInference\rModelling\rUpdating\rData Analysis\r\r…can be considered the same thing (certainly for the purposes of this post): the application of Bayes theorem to quantify uncertainty.","tags":["Bayesian","Stan","Decision Analysis"],"title":"Why Go Bayesian?","type":"post"},{"authors":[],"categories":["Bayesian","MCMC","R","Stan","Logistic Regression","Inspection Reliability","Probability of Detection"],"content":"\rTLDR\rLogistic regression is a popular machine learning model. One application of it in an engineering context is quantifying the effectiveness of inspection technologies at detecting damage. This post describes the additional information provided by a Bayesian application of logistic regression (and how it can be implemented using the Stan probabilistic programming language). Finally, I’ve also included some recommendations for making sense of priors.\n\rIntroductions\rSo there are a couple of key topics discussed here: Logistic Regression, and Bayesian Statistics. Before jumping straight into the example application, I’ve provided some very brief introductions below.\nBayesian Inference\rAt a very high level, Bayesian models quantify (aleatory and epistemic) uncertainty, so that our predictions and decisions take into account the ways in which our knowledge is limited or imperfect. We specify a statistical model, and identify probabilistic estimates for the parameters using a family of sampling algorithms known as Markov Chain Monte Carlo (MCMC). My preferred software for writing a fitting Bayesian models is Stan. If you are not yet familiar with Bayesian statistics, then I imagine you won’t be fully satisfied with that 3 sentence summary, so I will put together a separate post on the merits and challenges of applied Bayesian inference, which will include much more detail.\n\rLogistic Regression\rLogistic regression is used to estimate the probability of a binary outcome, such as Pass or Fail (though it can be extended for \u0026gt; 2 outcomes). This is achieved by transforming a standard regression using the logit function, shown below. The term in the brackets may be familiar to gamblers as it is how odds are calculated from probabilities. You may see logit and log-odds used exchangeably for this reason.\n\\[\rLogit (x) = \\log\\Bigg({\\frac{x}{1 - x}}\\Bigg)\r\\]\nSince the logit function transformed data from a probability scale, the inverse logit function transforms data to a probability scale. Therefore, as shown in the below plot, it’s values range from 0 to 1, and this feature is very useful when we are interested the probability of Pass/Fail type outcomes.\n\\[\rInverse\\;Logit (x) = \\frac{1}{1 + \\exp(-x)}\r\\]\nBefore moving on, some terminology that you may find when reading about logistic regression elsewhere:\n\rWhen a linear regression is combined with a re-scaling function such as this, it is known as a Generalised Linear Model (GLM).\rThe re-scaling (in this case, the logit) function is known as a link function in this context.\rLogistic regression is a Bernoulli-Logit GLM.\r\rYou may be familiar with libraries that automate the fitting of logistic regression models, either in Python (via sklearn):\nfrom sklearn.linear_model import LogisticRegression\rmodel = LogisticRegression()\rmodel.fit(X = dataset[\u0026#39;input_variables\u0026#39;], y = dataset[\u0026#39;predictions\u0026#39;])\r\r…or in R:\nmodel_fit \u0026lt;- glm(formula = preditions ~ input_variables,\rdata = dataset, family = binomial(link = \u0026#39;logit\u0026#39;))\r\r\r\rExample Application: Probability of Detection\rTo demonstrate how a Bayesian logistic regression model can be fit (and utilised), I’ve included an example from one of my papers. Engineers make use of data from inspections to understand the condition of structures. Modern inspection methods, whether remote, autonomous or manual application of sensor technologies, are very good. They are generally evaluated in terms of the accuracy and reliability with which they size damage. Engineers never receive perfect information from an inspection, such as:\n\rThere is a crack of exact length 30 mm and exact depth 5 mm at this exact location, or\rThere is definitely no damage at this location.\r\rFor various reasons, the information we receive from inspections is imperfect and this is something that engineers need to deal with. As a result, providers of inspection services are requested to provide some measure of how good their product is. This typically includes some measure of how accurately damage is sized and how reliable an outcome (detection or no detection) is.\nThis example will consider trials of an inspection tool looking for damage of varying size, to fit a model that will predict the probability of detection for any size of damage. Since various forms of damage can initiate in structures, each requiring inspection methods that are suitable, let’s avoid ambiguity and imagine we are only looking for cracks.\nDetecting Damage: Never 100% Reliable\n\rTest Data\rFor the purposes of this example we will simulate some data. Let’s imagine we have introduced some cracks (of known size) into some test specimens and then arranged for some blind trials to test whether an inspection technology is able to detect them.\nset.seed(1008)\rN \u0026lt;- 30; lower \u0026lt;- 0; upper \u0026lt;- 10; alpha_true \u0026lt;- -1; beta_true \u0026lt;- 1\rdepth \u0026lt;- runif(n = N, min = lower, max = upper)\rPoD_1D \u0026lt;- function(depth, alpha_1D, beta_1D){\rPoD \u0026lt;- exp(alpha_1D + beta_1D * log(depth)) / (1 + exp(alpha_1D + beta_1D * log(depth)))\rreturn (PoD)\r}\rpod_df \u0026lt;- data.frame(depth = depth, det = double(length = N))\rfor (i in seq(from = 1, to = nrow(pod_df), by = 1)) {\rpod_df$det[i] = rbernoulli(n = 1, p = PoD_1D(depth = pod_df$depth[i], alpha_1D = alpha_true, beta_1D = beta_true))\r}\rThe above code is used to create 30 crack sizes (depths) between 0 and 10 mm. We then use a log-odds model to back calculate a probability of detection for each. This is based on some fixed values for \\(\\alpha\\) and \\(\\beta\\). In a real trial, these would not be known, but since we are inventing the data we can see how successful our model ends up being in estimating these values.\nThe below plot shows the size of each crack, and whether or not it was detected (in our simulation). The smallest crack that was detected was 2.22 mm deep, and the largest undetected crack was 5.69 mm deep. Even so, it’s already clear that larger cracks are more likely to be detected than smaller cracks, though that’s just about all we can say at this stage.\nAfter fitting our model, we will be able to predict the probability of detection for a crack of any size.\nStan is a probabilistic programming language. In a future post I will explain why it has been my preferred software for statistical inference throughout my PhD.\nThe below is a simple Stan program to fit a Bayesian Probability of Detection (PoD) model:\ndata {\rint \u0026lt;lower = 0\u0026gt; N; // Defining the number of defects in the test dataset\rint \u0026lt;lower = 0, upper = 1\u0026gt; det [N]; // A variable that describes whether each defect was detected [1]or not [0]\rvector \u0026lt;lower = 0\u0026gt; [N] depth; // A variable that describes the corresponding depth of each defect\rint \u0026lt;lower = 0\u0026gt; K; // Defining the number of probabilistic predictions required from the model\rvector \u0026lt;lower = 0\u0026gt; [K] depth_pred;\r}\rparameters {\r// The (unobserved) model parameters that we want to recover\rreal alpha;\rreal beta;\r}\rmodel {\r// A logistic regression model relating the defect depth to whether it will be detected\rdet ~ bernoulli_logit(alpha + beta * log(depth));\r// Prior models for the unobserved parameters\ralpha ~ normal(0, 1);\rbeta ~ normal(1, 1);\r}\rgenerated quantities {\r// Using the fitted model for probabilistic predicition\r// K posterior predictive distributions will be estimated for a corresponding crack depth\rvector [K] postpred_pr;\rfor (k in 1:K) {\rpostpred_pr[k] = inv_logit(alpha + beta * log(depth_pred[k]));\r}\r}\rThe generated quantities block will be used to make predictions for the K values of depth_pred that we provide.\nK \u0026lt;- 50; depth_pred \u0026lt;- seq(from = lower, to = upper, length.out = K)\rThe above code generates 50 evenly spaced values, which we will eventually combine in a plot. In some instances we may have specific values that we want to generate probabilistic predictions for, and this can be achieved in the same way.\n\rFitting the model\rData can be pre-processed in any language for which a Stan interface has been developed. This includes, R, Python, and Julia. In this example we will use R and the accompanying package, rstan.\nOur Stan model is expecting data for three variables: N, det, depth, K and depth_pred and rstan requires this in the form of a list.\n\rResults\rOnce we have our data, and are happy with our model, we can set off the Markov chains. There are plenty of opportunities to control the way that the Stan algorithm will run, but I won’t include that here, rather we will mostly stick with the default arguments in rstan.\nlibrary(rstan)\rPoD_samples \u0026lt;- sampling(object = PoD_model, data = list(N = N, det = pod_df$det, depth = pod_df$depth,\rK = K, depth_pred = depth_pred), seed = 1008)\r## ## SAMPLING FOR MODEL \u0026#39;7e5f8dcf245c90341e1fcbe2d195277e\u0026#39; NOW (CHAIN 1).\r## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds\r## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r## Chain 1: Adjust your expectations accordingly!\r## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup)\r## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup)\r## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup)\r## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup)\r## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup)\r## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup)\r## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling)\r## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling)\r## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling)\r## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling)\r## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling)\r## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling)\r## Chain 1: ## Chain 1: Elapsed Time: 0.063 seconds (Warm-up)\r## Chain 1: 0.058 seconds (Sampling)\r## Chain 1: 0.121 seconds (Total)\r## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;7e5f8dcf245c90341e1fcbe2d195277e\u0026#39; NOW (CHAIN 2).\r## Chain 2: ## Chain 2: Gradient evaluation took 0 seconds\r## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r## Chain 2: Adjust your expectations accordingly!\r## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup)\r## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup)\r## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup)\r## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup)\r## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup)\r## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup)\r## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling)\r## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling)\r## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling)\r## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling)\r## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling)\r## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling)\r## Chain 2: ## Chain 2: Elapsed Time: 0.061 seconds (Warm-up)\r## Chain 2: 0.059 seconds (Sampling)\r## Chain 2: 0.12 seconds (Total)\r## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;7e5f8dcf245c90341e1fcbe2d195277e\u0026#39; NOW (CHAIN 3).\r## Chain 3: ## Chain 3: Gradient evaluation took 0 seconds\r## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r## Chain 3: Adjust your expectations accordingly!\r## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup)\r## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup)\r## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup)\r## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup)\r## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup)\r## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup)\r## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling)\r## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling)\r## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling)\r## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling)\r## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling)\r## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling)\r## Chain 3: ## Chain 3: Elapsed Time: 0.136 seconds (Warm-up)\r## Chain 3: 0.146 seconds (Sampling)\r## Chain 3: 0.282 seconds (Total)\r## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;7e5f8dcf245c90341e1fcbe2d195277e\u0026#39; NOW (CHAIN 4).\r## Chain 4: ## Chain 4: Gradient evaluation took 0 seconds\r## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r## Chain 4: Adjust your expectations accordingly!\r## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup)\r## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup)\r## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup)\r## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup)\r## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup)\r## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup)\r## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling)\r## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling)\r## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling)\r## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling)\r## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling)\r## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling)\r## Chain 4: ## Chain 4: Elapsed Time: 0.235 seconds (Warm-up)\r## Chain 4: 0.079 seconds (Sampling)\r## Chain 4: 0.314 seconds (Total)\r## Chain 4:\rNote:I’ve not included any detail here on the checks we need to do on our samples. There are some common challenges associated with MCMC methods, each with plenty of associated guidance on how to diagnose and resolve them. For now, let’s assume everything has gone to plan.\nNow, there are a few options for extracting samples from a stanfit object such as PoD_samples, including rstan::extract(). However, these usually require a little post-processing to get them into a tidy format - no big deal, but a hassle I’d rather avoid. That’s why I like to use the ggmcmc package, which we can use to create a data frame that specifies the iteration, parameter value and chain associated with each data point:\nlibrary(ggmcmc)\rPoD_extracted_samples \u0026lt;- ggs(S = PoD_samples)\rhead(x = PoD_extracted_samples, n = 5)\r## # A tibble: 5 x 4\r## Iteration Chain Parameter value\r## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 1 alpha -1.35 ## 2 2 1 alpha -1.13 ## 3 3 1 alpha -0.537\r## 4 4 1 alpha -0.304\r## 5 5 1 alpha -0.679\rWe have sampled from a 2-dimensional posterior distribution of the unobserved parameters in the model: \\(\\alpha\\) and \\(\\beta\\). Below is a density plot of their corresponding marginal distributions based on the 1000 samples collected from each of the 4 Markov chains that have been run.\nSo our estimates are beginning to converge on the values that were used to generate the data, but this plot also shows that there is still plenty of uncertainty in the results. Unlike many alternative approaches, Bayesian models account for the statistical uncertainty associated with our limited dataset - remember that we are estimating these values from 30 trials. These results describe the possible values of \\(\\alpha\\) and \\(\\beta\\) in our model that are consistent with the limited available evidence. If more data was available, we could expect the uncertainty in our results to decrease. I think there are some great reasons to keep track of this statistical (sometimes called epistemic) uncertainty - a primary example being that we should be interested in how confident our predictive models are in their own results!\r…but I’ll leave it at that for now, and try to stay on topic.\nHow do we know what do these estimates of \\(\\alpha\\) and \\(\\beta\\) mean for the PoD (what we are ultimately interested in)?\rWe can check this using the posterior predictive distributions that we have (thanks to the generated quantities block of the Stan program).\nOne thing to note from these results is that the model is able to make much more confident predictions for larger crack sizes. The increased uncertainty associated with shallow cracks reflects the lack of data available in this region - this could be useful information for a decision maker!\nThere are only 3 trials in our dataset considering cracks shallower than 3 mm (and only 1 for crack depths \u0026lt; 2 mm). If we needed to make predictions for shallow cracks, this analysis could be extended to quantify the value of future tests in this region.\n\rFinal Thought: Where Did Those Priors Come From and Are They Any Good?\rThere are many approaches for specifying prior models in Bayesian statistics. Weakly informative and MaxEnt priors are advocated by various authors. Unfortunately, Flat Priors are sometimes proposed too, particularly (but not exclusively) in older books. A flat prior is a wide distribution - in the extreme this would be a uniform distribution across all real numbers, but in practice distribution functions with very large variance parameters are sometimes used. In either case, a very large range prior of credible outcomes for our parameters is introduced the model. This may sound innocent enough, and in many cases could be harmless.\nFlat priors have the appeal of describing a state of complete uncertainty, which we may believe we are in before seeing any data - but is this really the case?\nPrior Expectations: Can We Do Better?\n\rSuppose you are using Bayesian methods to model the speed of some athletes. Even before seeing any data, there is some information that we can build into the model. For instance, we can discount negative speeds. We also wouldn’t need to know anything about the athletes to know that they would not be travelling faster than the speed of light. This may sound facetious, but flat priors are implying that we should treat all outcomes as equally likely. In fact, there are some cases where flat priors cause models to require large amounts of data to make good predictions (meaning we are failing to take advantage of Bayesian statistics ability to work with limited data).\nIn this example, we would probably just want to constrain outcomes to the range of metres per second, but the amount of information we choose to include is ultimately a modelling choice. Another helpful feature of Bayesian models is that the priors are part of the model, and so must be made explicit - fully visible and ready to be scrutinised.\nA common challenge, which was evident in the above PoD example, is lacking an intuitive understanding of the meaning of our model parameters. Here \\(\\alpha\\) and \\(\\beta\\) required prior models, but I don’t think there is an obvious way to relate their values to the result we were interested in. They are linear regression parameters on a log-odds scale, but this is then transformed into a probability scale using the logit function.\nThis problem can be addressed using a process known as Prior Predictive Simulation, which I was first introduced to in Richard McElreath’s fantastic book. This involves evaluating the predictions that our model would make, based only on the information in our priors. Relating our predictions to our parameters provides a clearer understanding of the implications of our priors.\nBack to our PoD parameters - both \\(\\alpha\\) and \\(\\beta\\) can take positive or negative values, but I could not immediately tell you a sensible range for them. Based on our lack of intuition it may be tempting to use a variance for both, right? Well, before making that decision, we can always simulate some predictions from these priors. The below code is creating a data frame of prior predictions for the PoD (PoD_pr) for many possible crack sizes.\r\\[\r\\alpha \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha})\r\\]\r\\[\r\\beta \\sim N(\\mu_{\\beta}, \\sigma_{\\beta})\r\\]\nx \u0026lt;- seq(from = min_depth, to = max_depth, length.out = N_samples)\rprPrSim_df \u0026lt;- data.frame(depth = x)\rfor (i in seq(from = 1, to = nrow(prPrSim_df), by = 1)) {\ralpha = rnorm(n = N_samples, mean = mu_alpha, sd = sigma_alpha)\rbeta = rnorm(n = N_samples, mean = mu_beta, sd = sigma_beta)\rprPrSim_df$PoD_pr[i] \u0026lt;- exp(alpha + beta * log(prPrSim_df$depth[i]))/(1 + exp(alpha + beta * log(prPrSim_df$depth[i])))\r}\rhead(prPrSim_df)\r## depth PoD_pr\r## 1 0.00000000 0.000000e+00\r## 2 0.01001001 4.085619e-05\r## 3 0.02002002 5.966123e-02\r## 4 0.03003003 4.766970e-05\r## 5 0.04004004 4.532144e-03\r## 6 0.05005005 2.997338e-03\rAnd we can visualise the information contained within our priors for a couple of different cases.\nOur wide, supposedly non-informative priors result in some pretty useless predictions. I’ve suggested some more sensible priors that suggest that larger cracks are more likely to be detected than small cracks, without overly constraining our outcome (see that there is still prior credible that very small cracks are detected reliably and that very large cracks are often missed).\nWhy did our predictions end up looking like this?\nBorrowing from McElreath’s explanation, it’s because \\(\\alpha\\) and \\(\\beta\\) are linear regression parameters on a log-odds (logit) scale. Since we are estimating a PoD we end up transforming out predictions onto a probability scale. Flat priors for our parameters imply that extreme values of log-odds are credible. All that prior credibility of values \u0026lt; - 3 and \u0026gt; 3 ends up getting concentrated at probabilities near 0 and 1. I think this is a really good example of flat priors containing a lot more information than they appear to.\nI’ll end by directing you towards some additional (generally non-technical) discussion of choosing priors, written by the Stan development team (link). It provides a definition of weakly informative priors, some words of warning against flat priors and more general detail than this humble footnote.\n\r\r","date":1581724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581762610,"objectID":"1e4147815e8d2849a58ac88626792334","permalink":"/post/bayesian-regression-models-with-stan/","publishdate":"2020-02-15T00:00:00Z","relpermalink":"/post/bayesian-regression-models-with-stan/","section":"post","summary":"TLDR\rLogistic regression is a popular machine learning model. One application of it in an engineering context is quantifying the effectiveness of inspection technologies at detecting damage. This post describes the additional information provided by a Bayesian application of logistic regression (and how it can be implemented using the Stan probabilistic programming language). Finally, I’ve also included some recommendations for making sense of priors.\n\rIntroductions\rSo there are a couple of key topics discussed here: Logistic Regression, and Bayesian Statistics.","tags":["Bayesian","MCMC","R","Stan","Logistic Regression","Inspection Reliability","Probability of Detection"],"title":"Bayesian Logistic Regression with Stan","type":"post"},{"authors":["Domenic Di Francesco","Michael Havbro Faber","Marios Chryssanthopoulos","Ujjwal Bharadwaj"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ( Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --- ","date":1576022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576022400,"objectID":"8c46068f4d544e126c455ee8009b21b3","permalink":"/publication/ifed/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/publication/ifed/","section":"publication","summary":"Extension of Bayesian decision theoretic framework to account for risk of inspection activities in quantifying the expected value of information.","tags":["Value of Information","Bayesian Decision Analysis"],"title":"Inspection Planning of Hazardous Locations Using a Value of Information Analysis","type":"publication"},{"authors":["Domenic Di Francesco","Michael Havbro Faber","Marios Chryssanthopoulos","Ujjwal Bharadwaj"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ( Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --- ","date":1568332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568332800,"objectID":"c26617cd10ccda065a0e66d0a59a98ff","permalink":"/publication/ipw/","publishdate":"2019-09-13T00:00:00Z","relpermalink":"/publication/ipw/","section":"publication","summary":"Quantifying the variability within, and the inter-dependency between all parameters in models that predict fatigue failure of welded steel structures.","tags":["Markov Chain Monte Carlo Sampling","Fracture \u0026 Fatigue","Bayesian Modelling"],"title":"Bayesian Fatigue Modelling","type":"publication"},{"authors":[],"categories":["R","gganimate"],"content":"\rTLDR\rThere are many instances where it may be useful to animate graphical representations of data, perhaps to add an additional dimension to a plot. The below example builds a cumulative map of car accidents in the UK using some of the functionality of the gganimate package.\n\rMaking Moving Plots with gganimate\rGraphics made using the ggplot2 package are already extremely customisable. They can be further enhanced using some of the extensions that have been developed. These include providing access to new themes, as well as entirely new functionality.\ngganimate allows for the animation of existing ggplot graphics. Once installed, we can load both packages (ggplot2 is included as part of the tidyverse):\nlibrary(gganimate); library(tidyverse)\rThe example uses a car accident dataset that I found on Kaggle. Here are the first few rows of the variables that we’re interested in:\nhead(Accidents_Dec2015 %\u0026gt;% dplyr::select(Date, Longitude, Latitude, Number_of_Casualties, Accident_Severity))\r## Date Longitude Latitude Number_of_Casualties Accident_Severity\r## 1 2015-12-01 -0.155880 51.48959 1 Slight\r## 2 2015-12-01 -0.200271 51.49262 1 Slight\r## 3 2015-12-03 -0.210643 51.49997 2 Slight\r## 4 2015-12-03 -0.156754 51.49293 1 Slight\r## 5 2015-12-03 -0.159124 51.50205 1 Slight\r## 6 2015-12-04 -0.197452 51.49104 1 Slight\rWe can plot the coordinates using a map of the UK included in ggplot2.\nUK_coords \u0026lt;- ggplot2::map_data(map = \u0026#39;world\u0026#39;, region = \u0026#39;UK\u0026#39;)\rBefore animating we need to create a ggplot that we will work from.\naccidents_plot \u0026lt;- ggplot(data = UK_coords)+\rgeom_polygon(mapping = aes(x = long, y = lat, group = group), col = \u0026#39;black\u0026#39;, fill = NA)+\rtheme_void(base_size = 12, base_family = \u0026#39;Bahnschrift\u0026#39;)+\rgeom_point(data = Accidents_Dec2015, mapping = aes(x = Longitude, y = Latitude, col = as.factor(Accident_Severity), alpha = as.factor(Accident_Severity), size = Number_of_Casualties,\rgroup = seq_len(length.out = nrow(Accidents_Dec2015))))+\rtheme(legend.position = \u0026#39;right\u0026#39;)+\rscale_size_continuous(breaks = c(1, 3, 9))+\rscale_color_manual(values = c(\u0026#39;firebrick\u0026#39;, \u0026#39;forestgreen\u0026#39;, \u0026#39;steelblue\u0026#39;))+\rscale_alpha_manual(values = c(0.4, 0.2, 0.1), guide = \u0026#39;none\u0026#39;)+\rguides(col = guide_legend(title = element_blank(), ncol = 1),\rsize = guide_legend(title = element_text(\u0026#39;Casualties\u0026#39;, size = 10), ncol = 1, alpha = 0.4))\raccidents_plot\rWe can now add some functions from gganimate, which will describe how and saving as a new variable:\nlibrary(gganimate)\raccidents_plot \u0026lt;- accidents_plot +\rtransition_time(time = Date)+\renter_grow()+\rshadow_mark()+\rggtitle(label = \u0026#39;UK Car Accidents in December 2015\u0026#39;, subtitle = \u0026#39;Date : {frame_time}\u0026#39;)+\rlabs(caption = \u0026#39;Data from Kaggle: https://www.kaggle.com/silicon99/dft-accident-data/data | @d73mwf\u0026#39;)\rtransition_time() requires specification of a time variable that the plot will display sequentially. As shown above, this animation will transition through values of Date. There are many more options that allow for animation across different data types in different ways.\nshadow_mark() has been added to keep accidents from previous dates. Again, there are various methods of showing data from previous states.\nenter_grow() means that when new data first appear on the plot, they will emerge by growing into their final size.\nSome of these additional options are detailed here.\nIn this case, transition_length and state_length describe the relative amount of time spent displaying the current state, and transitioning to the next state.\nRegardless of the transition function selected, the best way to create the moving plot is to use the animate function:\nanimate(plot = accidents_plot, fps = 20, duration = 30, end_pause = 100)\rThe animate() function requires us to specify the plot (to be animated), but includes many additional arguments not all of which are detailed above.\nfps is the frames per second, duration is the length of the animation (in seconds), and end_pause is the length of time that the final frame is held for (in number of frames.)\n\rFinal Animation\rI tried a few alternatives here. In this case each state has quite a few points and so I wanted it to be held for a reasonable amount of time. The trade-off here is the number of frames (and associated processing time and file size). The below allocates approximately 1 second per day and results in a total of 600 frames for the animation. On my (ageing) laptop this required approximately 4 minutes to render.\nAnimated Map\n\r\rThoughts on the Animation\rWhat does the animation tell us that the stationary plot doesn’t?\nThe final frame is pretty much identical, but the transitions will show when the accidents occurred. From viewing the animation there doesn’t appear to be a clear time when accidents were more frequent. We can check this with an additional plot:\nAccidents_Dec2015 %\u0026gt;% dplyr::select(Date, Number_of_Casualties, Accident_Severity) %\u0026gt;% ggplot(mapping = aes(x = Date))+\rgeom_bar(stat = \u0026#39;count\u0026#39;, fill = \u0026#39;grey80\u0026#39;)+\rfacet_wrap(facets = ~ Accident_Severity, scales = \u0026#39;free\u0026#39;)+\rtheme_ddf_light()+\rcoord_flip()+\rtheme(axis.title = element_blank())\rWhat isn’t shown in the data is the number of cars that were on the road at the time, so it could be that there were a higher proportion of accidents between the 24th and 31st December - but that will have to remain speculation for now.\nOne tip that I picked up from the package developer was the need to group data in the geom to avoid new points travelling from the location of other points. Given the context of the plot, that could have been interpreted as the same vehicles having accidents all over the UK. I’m glad that I was able to avoid this ambiguity in the plot.\nIn conclusion, I think the animated plot looks cool, but it is perhaps a little gimmicky for this particular application. The same information is contained in the two static plots in this post. However, I hope this has content serves as a helpful introduction to how the gganimate package can automate animated graphics.\n\rAdditional Resources\rThis example is certainly not exhaustive and there are many additional tweaks available to further customise an animation. I have personally found the below resources to be particularly helpful.\n\rThe official beginner’s guide to gganimate.\rA tutorial from the ISU Graphics Group.\r\r\r","date":1559606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559656867,"objectID":"9bf88e7a8c06ed5dff7fe1c6e16fcc3c","permalink":"/post/animating-plots/","publishdate":"2019-06-04T00:00:00Z","relpermalink":"/post/animating-plots/","section":"post","summary":"TLDR\rThere are many instances where it may be useful to animate graphical representations of data, perhaps to add an additional dimension to a plot. The below example builds a cumulative map of car accidents in the UK using some of the functionality of the gganimate package.\n\rMaking Moving Plots with gganimate\rGraphics made using the ggplot2 package are already extremely customisable. They can be further enhanced using some of the extensions that have been developed.","tags":["gganimate","dplyr","ggplot2","R"],"title":"Animating Plots","type":"post"},{"authors":null,"categories":["R","wordcloud"],"content":"\rTLDR\rWordclouds can be used to produce a neat summary of text and can readily be produced in R. This is a simple example based on a recent conferene paper.\n\rSummarising the content of a conference paper\rThere is an R package dedicated to creating wordclouds, so I’ve started by loading this, along with the tidyverse (for standard data manipulation) and tidytext (for some help processing the contents of the paper).\nlibrary(wordcloud); library(tidyverse); library(tidytext)\rThe wordcloud package creates a graphic of words that appear in some specified text. The size of the word is proprtional to its frequency in the text\rR can read text from a local file, as shown below, or from a website.\n# We can read a text file using \u0026#39;readLines\u0026#39; and we can select a file interactively using \u0026#39;file.choose\u0026#39;\r# Both of these are Base R functions\rpaper \u0026lt;- readLines(file.choose())\rThe ‘paper’ variable is currently as list of individual lines, as we can see when viewing one of its elements:\nprint(paper[2])\r## [1] \u0026quot;Application of MCMC Sampling to Account for Variability and Dependency\u0026quot;\rtidytext helps get this into a friendlier format allowing us to count the occirence of each word.\npaper_tbl \u0026lt;- as_tibble(paper) %\u0026gt;% tidytext::unnest_tokens(word, value) %\u0026gt;% dplyr::filter(is.na(as.numeric(word))) %\u0026gt;% count(word)\rSince I expected words like ‘the’ and ‘of’ are likely to feature a lot in the text, I wanted to be able to remove them. I initally used dplyr to set up a variable that would allow me to filter out shorter words, based on some threshold…\nminLength \u0026lt;- 4\rpaper_tbl \u0026lt;- paper_tbl %\u0026gt;% mutate(check = case_when(nchar(word) \u0026lt; minLength ~ 0,\rnchar(word) \u0026gt;= minLength ~ 1))\rBut then I learnt about stopwords and made use of the database that tidytext conveniently provides, before removing them from the data.\npaper_tbl \u0026lt;- paper_tbl %\u0026gt;% anti_join(tidytext::get_stopwords(language = \u0026#39;en\u0026#39;, source = \u0026#39;stopwords-iso\u0026#39;))\rBefore sending this directly into the wordcloud function, we can review the current state of the data, either as a table…\nhead(x = paper_tbl %\u0026gt;% arrange(desc(x = n)), n = 10)\r## # A tibble: 10 x 2\r## word n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 model 58\r## 2 models 27\r## 3 fatigue 25\r## 4 data 24\r## 5 parameters 24\r## 6 bayesian 22\r## 7 posterior 18\r## 8 crack 17\r## 9 growth 14\r## 10 priors 12\r…or as a simple plot (in either case I’m only interested in the most frequent words for now) …\nggplot(paper_tbl %\u0026gt;% arrange(desc(n)) %\u0026gt;% dplyr::filter(n \u0026gt;= 12))+\rgeom_col(mapping = aes(x = word, y = n))+\rtheme_minimal()+ theme(axis.text.x = element_text(angle = 90), axis.title.x = element_blank())+\rlabs(y = \u0026#39;count\u0026#39;)\rOne thing that is apparent from the above summaries is that we have not dealt with plurals from the data, i.e. ‘model’ and ‘models’ will be treated as two different words, with their own count. I’ve not found a neat way to combine these, but a manual solution with regular expressions (such as grepl, !grepl, etc.) would be simple, though not very elegant. I decided to leave plurals as they are.\nFinally, time to ask the wordcloud function to read and plot our data. There are some useful arguments to experiment with here:\n\rmin.freq and max.words set boundaries for how populated the wordcloud will be\rrandom.order will put the largest word in the middle if set to FALSE\rrot.per is the fraction of words that will be rotated in the graphic\r\rFinally, the words are arranged stochastically somehow, and so for a repeatable graphic we need to specify a seed value.\nset.seed(1008) wordcloud(words = paper_tbl$word, freq = paper_tbl$n, min.freq = 4, max.words = 100, random.order = FALSE, rot.per = 0.25,\rcolors = brewer.pal(n = 8, name = \u0026#39;Paired\u0026#39;))\rIf you’re not familiar with the colour palettes, the below line will ask R to display them for you:\nRColorBrewer::display.brewer.all()\rFinally, some links to more information regarding the packages introduced here, both of which are available on CRAN:\n\rwordcloud\rtidytext\r\r\r","date":1559606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559656867,"objectID":"fd5654dbf23a55a09d07145f00b3f06d","permalink":"/post/31.05.19-wordcloud/","publishdate":"2019-06-04T00:00:00Z","relpermalink":"/post/31.05.19-wordcloud/","section":"post","summary":"TLDR\rWordclouds can be used to produce a neat summary of text and can readily be produced in R. This is a simple example based on a recent conferene paper.\n\rSummarising the content of a conference paper\rThere is an R package dedicated to creating wordclouds, so I’ve started by loading this, along with the tidyverse (for standard data manipulation) and tidytext (for some help processing the contents of the paper).","tags":["R","Wordcloud","R Markdown"],"title":"Wordclouds","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]