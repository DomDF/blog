---
title: "Why Go Bayesian?"
author: ''
date: '2020-03-24'
categories:
- Bayesian
- R
- Stan
- Decision Analysis
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
lastmod: '2020-03-24T23:55:13Z'
projects: []
slug: why-go-bayesian
subtitle: A Non-Technical Discussion
summary: ''
tags:
- Bayesian
- R
- Stan
- Bayesian Decision Analysis
authors: []
---



<div id="tldr" class="section level3">
<h3>TLDR</h3>
<p>This post is intended to be a high-level discussion of the merits and challenges of applied Bayesian statistics. It is intended to help the reader answer: <em>Is it worth me learning Bayesian statistics?</em> or <em>Should I look into using Bayesian statistics in my project?</em> Maths, code and technical details have all been left out.</p>
<hr />
<div id="bayes" class="section level4">
<h4>Bayes</h4>
<div class="figure">
<img src="https://media.giphy.com/media/TJBbXQooivUNq/giphy.gif" alt="Bayes" />
<p class="caption">Bayes</p>
</div>
</div>
</div>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>Firstly, Bayesian…</p>
<ul>
<li>Statistics</li>
<li>Inference</li>
<li>Modelling</li>
<li>Updating</li>
<li>Data Analysis</li>
</ul>
<p>…can be considered the same thing (certainly for the purposes of this post): <strong>the application of Bayes theorem to quantify uncertainty</strong>.</p>
<p>So Bayesian statistics may be of interest to you if you are dealing with a problem associated with uncertainty - either due to some underlying variability, or due to limitations of your data.</p>
</div>
<div id="what-does-a-bayesian-approach-provide" class="section level3">
<h3>What does a Bayesian Approach Provide?</h3>
<p>Bayesian statistics is not the only way to account for uncertainty in calculations. The below points describe what Bayesian approach offers that others don’t. Note that I am only really discussing methods involving probability here, though <a href="https://www.springer.com/gp/book/9783540402947">alternative approaches are available</a>.</p>
<div id="interpretation-of-results" class="section level4">
<h4>Interpretation of Results</h4>
<p>The outcome of a Bayesian model is a posterior distribution. This describes the joint uncertainty in all the parameters we are trying to estimate. A posterior predictive distribution uses this information to describe uncertainty in a prediction that this model would make for some new input data. By comparison, alternative (frequentist) statistical analysis typically describes uncertainty in predictions using confidence intervals, which are widely used but easy to misinterpret.</p>
<p>Confidence intervals are calculated so that they will contain the <strong>true</strong> value of whatever you are trying to predict with some desired frequency. They provide no information (in the absence of additional assumptions) on how credible various possible results are. The Bayesian equivalent (sometimes called credible intervals) can be drawn anywhere on a predictive distribution. <a href="https://mitpress.mit.edu/books/introduction-statistical-decision-theory">Pratt, Raiffa and Schlaiffer</a> provide the following summary of the two approaches:</p>
<p><em>Imagine the plight of the manager who exclaims, ‘I understand [does he?] the meaning that the demand for XYZ will lie in the interval 973 to 1374 with confidence .90. However, I am particularly interested in the interval 1300 to 1500. What confidence can I place on that interval?’</em>
<em>Unfortunately, this question cannot be answered. Of course, however, it is possible to give a posterior probability to that particular interval - or any other - based on the sample data and on a codification of the manager’s prior judgements.</em></p>
<p>And a more succinct description of the same view is presented in <a href="https://www.weirdfishes.blog/">Dan Ovando’s fishery statistics blog</a>:</p>
<p><em>Bayesian credible intervals mean what we’d like Frequentist confidence intervals to mean.</em></p>
</div>
<div id="seamless-integration-with-decision-analysis" class="section level4">
<h4>Seamless Integration with Decision Analysis</h4>
<p>Following on from the previous point, an analysis that directly describes the probability of any outcome is fully compatible with a decision analysis. After completing a Bayesian analysis, identifying the optimal strategy implied by your model becomes simpler and more understandable.</p>
<p>As stated in <a href="https://www.springer.com/gp/book/9780387960982">James Berger’s (quite theoretical) book on Bayesian statistics</a>:</p>
<p><em>Bayesian analysis and decision theory go rather naturally together, partly because of their common goal of utilizing non-experimental sources of information, and partly because of deep theoretical ties.</em></p>
</div>
<div id="flexible-modelling" class="section level4">
<h4>Flexible Modelling</h4>
<p>So this one is based on a point made in <a href="https://uk.sagepub.com/en-gb/eur/a-student%E2%80%99s-guide-to-bayesian-statistics/book245409">Ben Lambert’s book on Bayesian statistics</a>. It is regarding how modern Bayesian statistics is achieved in practice The computational methods require some effort to pick up, especially if you do not have experience with programming (though Ben Lambert’s book gives a nice introduction to <a href="https://mc-stan.org/">Stan</a>). However, they can be extended to larger and more complex models in an intuitive way.</p>
<div class="figure">
<img src="https://media.giphy.com/media/WiyczarN2XMm4/giphy.gif" alt="Some Compelling Arguments" />
<p class="caption">Some Compelling Arguments</p>
</div>
</div>
</div>
<div id="challenges-difficulties" class="section level3">
<h3>Challenges &amp; Difficulties</h3>
<p>So why would anyone ever <em>not</em> use Bayesian models when making predictions?</p>
<div id="subjectivity" class="section level4">
<h4>Subjectivity</h4>
<p>Perhaps the most common criticism of Bayesian statistics is the requirement for prior models. An initial estimate of uncertainty is a term in Bayes’ theorem - but how can you estimate the extent of variability before you see it in your data? This will surely be completely subjective, so the results will vary depending on who is doing the analysis. This, understandably, doesn’t seem right with a lot of casual enquirers.</p>
<p>A common response to this accusation is that subjectivity is not an exclusive feature of Bayesian analysis (how about the whole structure of the model you are trying to fit, by any means?) <em>but</em> at least Bayesians are required to be explicit about it. Priors are part of the model with no-where to hide (in the code or the reporting) and so they are open to criticism. This point is discussed in <strong>much</strong> more detail in this paper from <a href="http://www.stat.columbia.edu/~gelman/research/published/gelman_hennig_full_discussion.pdf">Colombia University</a>.</p>
<p>Priors can contain, as much or as little, information as desired. However, even in instances where you may feel you don’t have any upfront knowledge of a problem, they represent a valuable opportunity for introducing regularisation (which protects against bad predictions due to overfitting). This idea is discussed in detail in <a href="https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919">Richard McElreath’s textbook</a>.</p>
</div>
<div id="computational-requirements" class="section level4">
<h4>Computational Requirements</h4>
<p>In practice, statisticians estimate Bayesian posterior distributions using Markov Chain Monte Carlo (MCMC) sampling algorithms. This approach is slower, more complicated and less informative than standard, independent Monte Carlo sampling. The models that I have worked with during my PhD have taken several hours to finish sampling from. Following this, there are checks that need to be completed as there are plenty of things that can go wrong with MCMC.</p>
<p>My background is in mechanical and civil engineering. In my discussions with engineering researchers at conferences I have often been told that the errors and complications they encountered when using MCMC methods had made them believe that Bayesian statistics wasn’t for them. These are challenges that I imagine everyone who has attempted modern Bayesian statistics will have encountered and resolving them requires a deeper understanding of your model. Both domain-specific and statistical knowledge is required to help ensure the model you are trying to fit is justified. In addition some programming <em>tricks</em> like reparameterisation can be of great help to your software, which sometimes needs equivalent, but easier to interpret instructions.</p>
<p>With all that in mind, when would this ever be worthwhile?</p>
</div>
</div>
<div id="conclusions" class="section level3">
<h3>Conclusions</h3>
<p>Regardless of whether you believe we exist in a deterministic universe or not, you will never have perfect state of knowledge describing your problem: uncertainty exists, so are you sure that your approach is sensible or safe?</p>
<p>I believe that Bayesian statistics is actually well suited to traditional engineering problems, as we are concerned with managing risk when confronted with small, messy datasets and models with plenty of uncertainty. As suggested in the earlier description of confidence interval, frequentist statistics defines probability based on occurrences of events following a large number of trials or samples. When only limited data is available, Bayesian statistics shines by comparison.</p>
<p>Very large datasets may contain enough information to precisely estimate parameters in a model using standard machine learning methods, and so it becomes less worthwhile running simulations to characterise variability. But how common are these big data problems in science and engineering? Sometimes large populations of data are better described as many smaller groups, after accounting for key differences between them. Bayesian statistics has a very useful way of managing such problems by structuring models hierarchically. This method allows for <strong>partial pooling of information</strong> between groups, so that predictions account for the variability and commonality between groups. I will provide a detailed example of this in a future post.</p>
<p>In conclusion, Bayesian statistics requires (computational and personal) effort to apply. But it provides results that are (usually) more interpretable and closely linked to questions we want to answer. Whether or not these methods are worth learning of course depend on your personal circumstances. I encountered Bayesian statistics during my PhD, and so had plenty of time to read up and I have personally found this to be very rewarding and enjoyable…</p>
<div class="figure">
<img src="https://media.giphy.com/media/WPLPEu0GUp41W/giphy.gif" alt="Boring, isn’t it? Writing, Fitting and Evaluating Bayesian Models All Day…." />
<p class="caption">Boring, isn’t it? Writing, Fitting and Evaluating Bayesian Models All Day….</p>
</div>
</div>
