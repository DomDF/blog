---
title: Maximum Likelihood Estimation
author: ''
date: '2020-07-28'
slug: maximum-likelihood-estimation
categories:
  - Frequentist Statistics
  - Parameter Estimation
  - Maximum Likelihood Estimation
  - R
  - Python
  - SciPy
tags:
  - Frequentist Statistics
  - Parameter Estimation
  - Maximum Likelihood Estimation
  - R
  - Python
  - SciPy
subtitle: ''
summary: ''
authors: []
lastmod: '2020-07-27T15:45:32+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<div id="tldr" class="section level3">
<h3>TLDR</h3>
<p>Maximum Likelihood Estimation (MLE) is one method of inferring model parameters. This post aims to give an intuitive explanation of MLE, discussing why it is so useful (simplicity and availability in software) as well as where it is limited (point estimates are not as informative as Bayesian estimates, which are also shown for comparison).</p>
<hr />
</div>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>Distribution parameters describe the shape of a distribution function. A normal (Gaussian) distribution is characterised based on it’s mean, <span class="math inline">\(\mu\)</span> and standard deviation, <span class="math inline">\(\sigma\)</span>. Increasing the mean <em>shifts</em> the distribution to be centered at a larger value and increasing the standard deviation <em>stretches</em> the function to give larger values further away from the mean. When we approximate some uncertain data with a distribution function, we are interested in estimating the distribution parameters that are most consistent with the data.</p>
<p>The likelihood, <span class="math inline">\(L\)</span>, of some data, <span class="math inline">\(z\)</span>, is shown below. Where <span class="math inline">\(f(\theta)\)</span> is the function that has been proposed to explain the data, and <span class="math inline">\(\theta\)</span> are the parameter(s) that characterise that function.</p>
<p><span class="math display">\[
L = \displaystyle\prod_{i=1}^{N} f(z_{i} \mid \theta)
\]</span></p>
<p>Likelihood values (and therefore also the product of many likelihood values) can be very small, so small that they cause problems for software. Therefore it’s usually more convenient to work with log-likelihoods instead. Taking the logarithm is applying a <em>monotonically increasing</em> function. This means if one function has a higher sample likelihood than another, then it will also have a higher log-likelihood. Also, the location of maximum log-likelihood will be also be the location of the maximum likelihood.</p>
<p><span class="math display">\[
\log{(L)} = \displaystyle\sum_{i=1}^{N} f(z_{i} \mid \theta)
\]</span></p>
<p>The distribution parameters that maximise the log-likelihood function, <span class="math inline">\(\theta^{*}\)</span>, are those that correspond to the maximum sample likelihood.</p>
<p><span class="math display">\[
\theta^{*} = arg \max_{\theta} \bigg[ \log{(L)} \bigg]
\]</span></p>
<p>Below, two different normal distributions are proposed to describe a pair of observations.</p>
<pre class="r"><code>obs &lt;- c(0, 3)</code></pre>
<p>The red distribution has a mean value of <code>1</code> and a standard deviation of <code>2</code>. The green distribution has a mean value of <code>2</code> and a standard deviation of <code>1</code> and so is centered further to the right, and is less dispersed (less stretched out). The red arrows point to the likelihood values of the data associated with the red distribution, and the green arrows indicate the likelihood of the same data with respect to the green function. The first data point, 0 is more likely to have been generated by the red function, and the second data point, 3 is more likely to have been generated by the green function.</p>
<p><img src="/post/2020-07-28-maximum-likelihood-estimation_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We can evaluate the log-likelihood and compare the two functions:</p>
<pre class="r"><code>sum(log(dnorm(x = obs, mean = 1, sd = 2))) # Red function
## [1] -3.849171

sum(log(dnorm(x = obs, mean = 2, sd = 1))) # Green function
## [1] -4.337877</code></pre>
<p>As shown above, the red distribution has a higher log-likelihood (and therefore also a higher likelihood) than the green function, with respect to the 2 data points. The above graph suggests that this is driven by the first data point , 0 being significantly more consistent with the red function. The below example looks at how a distribution parameter that maximises a sample likelihood could be identified.</p>
</div>
<div id="mle-for-an-exponential-distribution" class="section level3">
<h3>MLE for an Exponential Distribution</h3>
<p>The exponential distribution is characterised by a single parameter, it’s rate <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[
f(z, \lambda) = \lambda \cdot \exp^{- \lambda \cdot z}
\]</span></p>
<p>It is a widely used distribution, as it is a Maximum Entropy (MaxEnt) solution. If some unknown parameters is known to be positive, with a fixed mean, then the function that best conveys this (and only this) information is the exponential distribution. I plan to write a future post about the MaxEnt principle, as it is deeply linked to Bayesian statistics. The expectation (mean), <span class="math inline">\(E[y]\)</span> and variance, <span class="math inline">\(Var[y]\)</span> of an exponentially distributed parameter, <span class="math inline">\(y \sim exp(\lambda)\)</span> are shown below:</p>
<p><span class="math display">\[
E[y] = \lambda^{-1}, \; Var[y] = \lambda^{-2}
\]</span></p>
<p>Simulating some example data…</p>
<pre class="r"><code>n_samples &lt;- 25; true_rate &lt;- 1; set.seed(1)

exp_samples &lt;- rexp(n = n_samples,
                    rate = true_rate)</code></pre>
<p>In the above code, 25 independent random samples have been taken from an exponential distribution with a mean of 1, using <code>rexp</code>.</p>
<p>Below, for various proposed <span class="math inline">\(\lambda\)</span> values, the log-likelihood (<code>log(dexp())</code>) of the sample is evaluated.</p>
<pre class="r"><code>
exp_lik_df &lt;- data.frame(rate = double(), 
                         lik = double())

for (i in seq(from = 0.2, to = 2, by = 0.2)){
  
  exp_lik_df &lt;- rbind(exp_lik_df, 
                      data.frame(rate = i,
                                 log_lik = sum(log(
                                   dexp(x = exp_samples,
                                        rate = i)))))
  
}

max_log_lik &lt;- exp_lik_df[which.max(x = exp_lik_df$log_lik),]</code></pre>
<p>Finally, <code>max_log_lik</code> finds which of the proposed <span class="math inline">\(\lambda\)</span> values is associated with the highest log-likelihood.</p>
<p>We can print out the data frame that has just been created and check that the maximum has been correctly identified.</p>
<pre class="r"><code>print(exp_lik_df)
##    rate   log_lik
## 1   0.2 -45.38774
## 2   0.4 -33.21086
## 3   0.6 -28.22602
## 4   0.8 -26.18577
## 5   1.0 -25.75897
## 6   1.2 -26.35273
## 7   1.4 -27.65076
## 8   1.6 -29.46427
## 9   1.8 -31.67149
## 10  2.0 -34.18927

print(max_log_lik)
##   rate   log_lik
## 5    1 -25.75897</code></pre>
<p>The below plot shows how the sample log-likelihood varies for different values of <span class="math inline">\(\lambda\)</span>. It also shows the shape of the exponential distribution associated with the lowest (top-left), optimal (top-centre) and highest (top-right) values of <span class="math inline">\(\lambda\)</span> considered in these iterations:</p>
<p><img src="/post/2020-07-28-maximum-likelihood-estimation_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div id="mle-in-practice-software-libraries" class="section level4">
<h4>MLE in Practice: Software Libraries</h4>
<p>In practice there are many software packages that quickly and conveniently automate MLE. Here are some useful examples…</p>
<p>Firstly, using the <code>fitdistrplus</code> library in <code>R</code>:</p>
<pre class="r"><code>
library(fitdistrplus)

sample_data &lt;- exp_samples

rate_fit_R &lt;- fitdist(data = sample_data, 
                      distr = &#39;exp&#39;, 
                      method = &#39;mle&#39;)</code></pre>
<p>Although I have specified <code>mle</code> (maximum likelihood estimation) as the method that I would like <code>R</code> to use here, it is already the default argument and so we didn’t need to include it.</p>
<p><code>R</code> provides us with an list of plenty of useful information, including:
- the original data
- the size of the dataset
- the co-variance matrix (especially useful if we are estimating multiple parameters)
- some measures of well the parameters were estimated</p>
<p>You can explore these using <code>$</code> to check the additional information available.</p>
<p>We can take advantage of this to extract the estimated parameter value and the corresponding log-likelihood:</p>
<pre class="r"><code>rate_fit_R$estimate
##      rate 
## 0.9705356
rate_fit_R$loglik
## [1] -25.74768</code></pre>
<p>Alternatively, with <code>SciPy</code> in <code>Python</code> (using the same data):</p>
<pre class="python"><code>import scipy.stats as stats

sample_data = r.exp_samples

rate_fit_py = stats.expon.fit(data = sample_data, floc = 0)
rate = (rate_fit_py[1])**-1

print(rate)
## 0.9705355729681481</code></pre>
<p>Though we did not specify MLE as a method, the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html">online documentation</a> indicates this is what the function uses.</p>
<p>We can also calculate the log-likelihood associated with this estimate using <code>NumPy</code>:</p>
<pre class="python"><code>import numpy as np

np.sum(np.log(stats.expon.pdf(x = sample_data, 
                              scale = rate_fit_py[1])))
## -25.747680569393435</code></pre>
<p>We’ve shown that values obtained from <code>Python</code> match those from <code>R</code>, so (as usual) both approaches will work out.</p>
<p>The <code>method</code> argument in <code>R</code>’s <code>fitdistrplus::fitdist()</code> function also accepts <code>mme</code> (moment matching estimation) and <code>qme</code> (quantile matching estimation), but remember that MLE is the default. One useful feature of MLE, is that (with sufficient data), parameter estimates can be approximated as normally distributed, with the covariance matrix (for all of the parameters being estimated) equal to the inverse of the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a> of the likelihood function.</p>
<p>However, MLE is primarily used as a point estimate solution and the information contained in a single value will always be limited. Likelihoods will not necessarily be symmetrically dispersed around the point of maximum likelihood. We may be interested in the full distribution of credible parameter values, so that we can perform sensitivity analyses and understand the possible outcomes or optimal decisions associated with particular credible intervals. See below for a proposed approach for overcoming these limitations.</p>
</div>
</div>
<div id="limitations-how-to-do-better-wit-bayesian-methods" class="section level3">
<h3>Limitations (‘How to do better wit Bayesian methods’)</h3>
<p>An intuitive method for quantifying this <em>epistemic</em> (statistical) uncertainty in parameter estimation is Bayesian inference. This removes requirements for a sufficient sample size, while providing more information (a full <em>posterior</em> distribution) of credible values for each parameter. If multiple parameters are being simultaneously estimated, then the posterior distribution will be a joint probabilistic model of all parameters, accounting for any inter-dependencies too. Finally, it also provides the opportunity to build in prior knowledge, which we may have available, before evaluating the data.</p>
<p>Returning to the challenge of estimating the rate parameter for an exponential model, based on the same 25 observations:</p>
<pre class="r"><code>summary(exp_samples)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.1061  0.4361  0.7552  1.0304  1.2296  4.4239</code></pre>
<p>We will now consider a Bayesian approach, by writing a Stan file that describes this exponential model:</p>
<pre class="stan"><code>data {

  int &lt;lower = 0&gt; N; // Defining the number of observations
  vector &lt;lower = 0&gt; [N] samples; // A vector containing the observations
  
}

parameters {
  
  // The (unobserved) model parameter that we want to estimate
  real &lt;lower = 0&gt; rate;

}

model {

  // An exponential model, which we are proposing to describe the data
  samples ~ exponential(rate);
  
}
</code></pre>
<p>As with <a href="https://www.allyourbayes.com/post/bayesian-regression-models-with-stan/">previous examples</a> on this blog, data can be pre-processed, and results can be extracted using the <code>rstan</code> package:</p>
<pre class="r"><code>library(rstan)

exp_posterior_samples &lt;- sampling(object = exp_model,
                                  data = list(N = n_samples, 
                                              samples = exp_samples),
                                  seed = 1008)
## 
## SAMPLING FOR MODEL &#39;5328d75314acb3313bc7fa418eeb08c2&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.025 seconds (Warm-up)
## Chain 1:                0.021 seconds (Sampling)
## Chain 1:                0.046 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;5328d75314acb3313bc7fa418eeb08c2&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.032 seconds (Warm-up)
## Chain 2:                0.023 seconds (Sampling)
## Chain 2:                0.055 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;5328d75314acb3313bc7fa418eeb08c2&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.026 seconds (Warm-up)
## Chain 3:                0.027 seconds (Sampling)
## Chain 3:                0.053 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;5328d75314acb3313bc7fa418eeb08c2&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.027 seconds (Warm-up)
## Chain 4:                0.039 seconds (Sampling)
## Chain 4:                0.066 seconds (Total)
## Chain 4:</code></pre>
<p><strong>Note</strong>: We have not specified a prior model for the rate parameter. <code>Stan</code> responds to this by setting what is known as an <em>improper</em> prior (a uniform distribution bounded only by any upper and lower limits that were listed when the parameter was declared). For real-world problems, there are many reasons to avoid uniform priors. Partly because they are no longer ‘non-informative’ when there are transformations, such as in generalised linear models, and partly because there will always be some prior information to help direct you towards more credible outcomes. However, this data has been introduced without any context and by using uniform priors, we should be able to recover the same maximum likelihood estimate as the non-Bayesian approaches above.</p>
<p>Extracting the results from our model:</p>
<pre class="r"><code>library(ggmcmc)

extracted_samples &lt;- ggs(S = exp_posterior_samples)

head(x = extracted_samples, n = 5)
## # A tibble: 5 x 4
##   Iteration Chain Parameter value
##       &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;     &lt;dbl&gt;
## 1         1     1 rate      0.614
## 2         2     1 rate      0.710
## 3         3     1 rate      0.783
## 4         4     1 rate      1.00 
## 5         5     1 rate      1.16</code></pre>
<p>We can use this data to visualise the uncertainty in our estimate of the rate parameter:</p>
<pre class="r"><code>ggplot(data = extracted_samples %&gt;% 
         dplyr::filter(Parameter == &#39;rate&#39;))+
  geom_density(mapping = aes(x = value, 
                             y = ..density..), 
               fill = &#39;purple4&#39;, alpha = 0.2)+
  geom_vline(aes(lty = &#39;MLE solution&#39;, 
                 xintercept = rate_fit_R$estimate))+
  scale_linetype_manual(values = c(2))+
  scale_x_continuous(name = &#39;Rate Parameter&#39;)+
  scale_y_continuous(name = &#39;Posterior Likelihood&#39;)+
  theme_ddf_light()</code></pre>
<p><img src="/post/2020-07-28-maximum-likelihood-estimation_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We can use the full posterior distribution to identify the maximum posterior likelihood (which matches the MLE value for this simple example, since we have used an improper prior). However, we can also calculate credible intervals, or the probability of the parameter exceeding any value that may be of interest to us.</p>
<p>This distribution includes the statistical uncertainty due to the limited sample size. As more data is collected, we generally see a reduction in uncertainty. Based on a similar principle, if we had also have included some information in the form of a prior model (even if it was only weakly informative), this would also serve to reduce this uncertainty.</p>
<p>Finally, we can also sample from the posterior distribution to plot predictions on a more meaningful outcome scale (where each green line represents an exponential model associated with a single sample from the posterior distribution of the rate parameter):</p>
<p><img src="/post/2020-07-28-maximum-likelihood-estimation_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
