---
title: Conceptual Introduction - Information Entropy
author: ''
date: '2019-06-11'
categories:
  - Bayesian
  - Entropy
  - Information
  - R
tags:
  - Bayesian
  - Entropy
  - Information
  - R
slug: 
lastmod: '2019-06-11T09:48:07+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

### TLDR

Entropy is used synonymously with disorder. For instance, it is blamed for headphone cables tangling when left in a bag. If we consider two macro-states of a headphone cable: *tangled* and *untangled*, then think about possible micro-states, i.e. every possible arrangement they could conceivably be forced into, it is evident that there are more ways that the cable can exist in a tangled state than untangled. Therefore, if allowed to move randomly (even a little) amongst other contents in a bag, they become tangled.

In the context of statistics, **information entropy** can be used to work with uncertainty. Methods of Maximising Entropy (MaxEnt), conditional on some given costraints, produce models that are most likely to be consistent with future observations. These arguments are used to support modelling choices in statistical decision analysis. Information theory is also the basis for various methods of evaluating the relative performance of *competing* models.

---

### Thermodynamic Entropy

As an engineering undergraduate student, I was taught about entropy in the context of thermodynamics. The reason processes like expanding steam through a turbine are irreversible is because they are not isentropic. They are associated with an increase in entropy, and a the entropy of a system can never decrease.

Include picture of a T-s diagram
What is an isentropic process? (A reversible process)

Not possible -> second law of thermodynamics.

Slight digression, but will pay off soon:

Quantitative case: moving molecules.

What is the system? What are possible *macro-states*?  What are possible *micro-states*?

How many different ways can each macro-state exist? Depends on the number of possible micro-states!
The most likely outcome in such a stochastic process is the outcome that can occur in the most ways. This is the maximum entropy (**MaxEnt**) solution.

Unless some energy is put into the system to encourage a particular macro-state, the MaxEnt argument is the best bet.


for $N$ gas molecules each of a distinct energy level, $E$, where $p_{i}$ is the proportion of molecules with energy $E_{i}$

\[
S = -k \cdot \sum\limits_{i = 1}^{E} p_{i} \cdot log(p_{i})
\]

The above expression can be used to maximise $S$. 


This sort of description of entropy is no longer restricted to thermodynamic systems and can also be applied to uncertainty.

### Information Entropy

Information theory is the formal framework for modelling uncertainty. [Claude Shannon](link to bio) is regarded as a pioneer in the field, describing how information is communicated mathematically. His work became the basis for modern telecommunication systems.

\[
H = -k \cdot \sum\limits_{i = 1}^{n} p_{i} \cdot log(p_{i})
\]

looks familiar, right?

The best introductions to information entropy that I have read are in books by [I.Jordaan](https://www.amazon.co.uk/Decisions-under-Uncertainty-Probabilistic-Engineering/dp/0521782775/ref=sr_1_1?keywords=decision+making+under+uncertainty+jordaan&qid=1560247665&s=books&sr=1-1), [R.McElreath](https://www.amazon.co.uk/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445/ref=sr_1_1?crid=3NY20Q0R1GOYA&keywords=statistical+rethinking&qid=1560247633&s=books&sprefix=statistical+re%2Cstripbooks%2C138&sr=1-1) and [E.T. Jaynes](https://www.amazon.co.uk/Probability-Theory-Principles-Elementary-Applications/dp/0521592712), who each provide helpful anaologies.

#### Analogy 1 (McElreath)

Jaynes uses the example of morbid children's word-game **hangman**.


McElreath suggests an experiment where ten balls are randomly placed in five buckets, and evaluates a few different arrangements.

The two extremes are:
  * All ten balls are placed in a single bucket (e.g Bucket 1 of 5): There is only one way of achieving this.
  * Two balls are placed in each of the five buckets: There are 113400 ways of achieving this.
  

#### Analogy 2 (Jordaan)

Jordaan proposes a simple definition of entropy:

*'Entropy is the smallest expected number of YESâ€“NO questions to determine the true event.'*

...which I think can be interpretted as the complexity of a system.
  
He elaborates on this idea by imagining an **entropy demon** playing games. In one instance, he plays a game of roulette many times.






In each of the above examples we are relating information to a count of possible system states.

### MaxEnt Statistical Models

MaxEnt methods are also used in ecology, and [J. Harte and E. Newman](https://www.sciencedirect.com/science/article/pii/S0169534714001037) have written an introduction, which also includes statistical distributions which are MaxEnt solutions for some given constraints.

If we were to bet on the outcome of the number of balls in each of McElreath's buckets, the optimal way to do so would be to use a uniform distribution. The MaxEnt solution (the macro state consistent with the largest number of possible micro-states) is a uniform distribution. Selecting any other distribution to model the probability of the number of balls in the buckets


Bayesian Inference as an extension of MaxEnt methodology ...

A discussion on Bayesian models will follow.

```{r, include = TRUE, eval = FALSE, warning = FALSE, message = FALSE}
# We can read a text file using 'readLines' and we can select a file interactively using 'file.choose'
# Both of these are Base R functions
paper <- readLines(file.choose())
```


MaxEnt in ecology


Congratulations on making it to the end!

```{r, message = FALSE, warning = FALSE, echo = FALSE}

library(giphyr)


```

![entropy](https://media3.giphy.com/media/xTiTnwqkkEtj3hm6DS/giphy.gif)  
<div style='font-size:50%'>([GIF Source link](www.behance.net/banaszek), accessed Sep 17, 2019)</div>  


[XKCD license](https://xkcd.com/license.html])
